{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ebed7c43-fa94-46f4-b5ce-3a193697dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Dependencies\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from river import tree, naive_bayes\n",
    "from river.datasets import synth\n",
    "import matplotlib.pyplot as plt\n",
    "from river import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from capymoa.stream.generator import LEDGeneratorDrift, SEA, RandomRBFGeneratorDrift\n",
    "from capymoa.stream.drift import DriftStream, AbruptDrift, GradualDrift\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  CONFIG - same as pipeline 2\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TOTAL_SAMPLES   = 1_000_000          # change for quick tests\n",
    "TRAIN_RATIO     = 0.80\n",
    "NUM_CLASSES     = 10\n",
    "INPUT_DIM       = 24                 # 7 relevant + 17 irrelevant\n",
    "BATCH           = 256\n",
    "EPOCHS          = 75\n",
    "LR              = 2e-3\n",
    "SEED_STREAM     = 112\n",
    "SEED_TORCH      = 42\n",
    "torch.manual_seed(SEED_TORCH)\n",
    "random.seed(SEED_TORCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5d80c6b1-ba9c-4451-873a-c9bbfa658a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from capymoa.stream import MOAStream, Stream\n",
    "from moa.streams import ConceptDriftStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4081155f-d46b-4d2a-8bad-692fadd43fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "led_g_stream = DriftStream(\n",
    "    stream=[LEDGeneratorDrift(number_of_attributes_with_drift=1),\n",
    "    \n",
    "    GradualDrift(width=50_000, position=225_000),\n",
    "   \n",
    "    LEDGeneratorDrift(number_of_attributes_with_drift=3),\n",
    "    GradualDrift(width=50_000, position=475_000),\n",
    "    \n",
    "    LEDGeneratorDrift(number_of_attributes_with_drift=5),\n",
    "    GradualDrift(width=50_000, position=725_000),\n",
    "    \n",
    "    LEDGeneratorDrift(number_of_attributes_with_drift=7)\n",
    "           ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d2fd49ba-6706-46e6-8e69-965ba3a8be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "led_a_stream = DriftStream(\n",
    "    stream=[LEDGeneratorDrift(number_of_attributes_with_drift=1),\n",
    "    \n",
    "    GradualDrift(width=50, position=249_975),\n",
    "   \n",
    "    LEDGeneratorDrift(number_of_attributes_with_drift=3),\n",
    "    GradualDrift(width=50, position=499_975),\n",
    "    \n",
    "    LEDGeneratorDrift(number_of_attributes_with_drift=5),\n",
    "    GradualDrift(width=50, position=749_975),\n",
    "    \n",
    "    LEDGeneratorDrift(number_of_attributes_with_drift=7)\n",
    "           ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ce2478b2-ed9f-44a8-b1cd-d812010c7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_a_stream = DriftStream(\n",
    "    stream=[SEA(function=1),\n",
    "            GradualDrift(width=50, position=249_975),\n",
    "            SEA(function=2),\n",
    "            GradualDrift(width=50, position=499_975),\n",
    "            SEA(function=4),\n",
    "            GradualDrift(width=50, position=749_975),\n",
    "            SEA(function=1)\n",
    "           ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b5c5de0f-2adb-447b-bad3-e93fcd099c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sea_g_stream = DriftStream(\n",
    "    stream=[SEA(function=1),\n",
    "    \n",
    "    GradualDrift(width=50_000, position=225_000),\n",
    "   \n",
    "    SEA(function=2),\n",
    "    GradualDrift(width=50_000, position=475_000),\n",
    "    \n",
    "    SEA(function=4),\n",
    "    GradualDrift(width=50_000, position=725_000),\n",
    "    SEA(function=1)\n",
    "    \n",
    "           ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "64cdaf3a-c8a3-4b26-8097-a65b4a15ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_m_stream = RandomRBFGeneratorDrift(number_of_drifting_centroids=50, magnitude_of_change=0.0001)\n",
    "rbf_f_stream = RandomRBFGeneratorDrift(number_of_drifting_centroids=50, magnitude_of_change=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "17402c57-aed6-430a-8c3a-ccca03edb01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading covtype.arff\n",
      "100% [........................................................................] 11241262 / 11241262"
     ]
    }
   ],
   "source": [
    "from capymoa.datasets import Electricity, Covtype\n",
    "\n",
    "elec_stream = Electricity()\n",
    "covt_stream = Covtype()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3b7d76c2-32c4-4b34-b5e8-81b409d6e09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@relation 'generators.LEDGeneratorDrift '\n",
       "\n",
       "@attribute att1 {0,1}\n",
       "@attribute att2 {0,1}\n",
       "@attribute att3 {0,1}\n",
       "@attribute att4 {0,1}\n",
       "@attribute att5 {0,1}\n",
       "@attribute att6 {0,1}\n",
       "@attribute att7 {0,1}\n",
       "@attribute att8 {0,1}\n",
       "@attribute att9 {0,1}\n",
       "@attribute att10 {0,1}\n",
       "@attribute att11 {0,1}\n",
       "@attribute att12 {0,1}\n",
       "@attribute att13 {0,1}\n",
       "@attribute att14 {0,1}\n",
       "@attribute att15 {0,1}\n",
       "@attribute att16 {0,1}\n",
       "@attribute att17 {0,1}\n",
       "@attribute att18 {0,1}\n",
       "@attribute att19 {0,1}\n",
       "@attribute att20 {0,1}\n",
       "@attribute att21 {0,1}\n",
       "@attribute att22 {0,1}\n",
       "@attribute att23 {0,1}\n",
       "@attribute att24 {0,1}\n",
       "@attribute class {0,1,2,3,4,5,6,7,8,9}\n",
       "\n",
       "@data"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = led_g_stream.get_schema()\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "02f2f917-0fb4-45b4-b6ee-552a676a3de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@relation 'generators.SEAGenerator '\n",
       "\n",
       "@attribute attrib1 numeric\n",
       "@attribute attrib2 numeric\n",
       "@attribute attrib3 numeric\n",
       "@attribute class {groupA,groupB}\n",
       "\n",
       "@data"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sea_schema = sea_g_stream.get_schema()\n",
    "sea_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b86349de-1e91-448b-b17e-4bccd83e8cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledInstance(\n",
       "    Schema(generators.LEDGeneratorDrift ),\n",
       "    x=[1. 1. 0. ... 0. 0. 0.],\n",
       "    y_index=5,\n",
       "    y_label='5'\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "led_g_stream.next_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e9b73d3e-d908-4585-af7e-493fcb1d8e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x):\n",
    "    return torch.tensor(x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b7529-cc90-477a-b5dd-d161c0c894c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40b289-a750-4222-928f-2f1f9980ce47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc38201-4788-4af9-9ca0-5f2ab71904ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a8843-1d81-4bdf-80f0-a639f26ec0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3958ad3d-e07d-4754-bdb6-58643ca62318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1.  Hyper-params & boiler-plate\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "from capymoa.classifier import HoeffdingTree\n",
    "\n",
    "\n",
    "n_experts = 15\n",
    "\n",
    "TOP_K         = 3            # update the K heaviest-weighted experts\n",
    "PRINT_EVERY   = 10_000\n",
    "CLASSES       = list(range(NUM_CLASSES))\n",
    "\n",
    "def to_tensor(x):\n",
    "    return torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2.  Initialise experts and router\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "class RouterMLP(nn.Module):\n",
    "    def __init__(self, in_dim=INPUT_DIM, h=256, out_dim=n_experts):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, h), nn.ReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(h, h // 2), nn.ReLU(),\n",
    "            nn.Linear(h // 2, out_dim)\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "e409bb16-c8dd-47dd-9384-f15c2580bfbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3 and 24x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[229], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m x_t   \u001b[38;5;241m=\u001b[39m to_tensor(x_vec)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)         \u001b[38;5;66;03m# 1Ã—24\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# 3-B  Router forward\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m logits  \u001b[38;5;241m=\u001b[39m \u001b[43mrouter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 1Ã—n_experts\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#tau0, tau_min, decay_steps = 2.0, 0.7, 80_000   \u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#tau = max(tau_min, tau0 * (1 - t / decay_steps)) # linear-cosine also works\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#weights = torch.softmax(logits / tau, dim=1)     # replaces previous softmax                       \u001b[39;00m\n\u001b[1;32m     32\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)        \u001b[38;5;66;03m# 1Ã—n_experts\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/moe_paper/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[228], line 33\u001b[0m, in \u001b[0;36mRouterMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/moe_paper/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/moe_paper/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/moe_paper/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/moe_paper/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3 and 24x256)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3.  Online joint-training loop (basic version)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "experts = {i: HoeffdingTree(schema=schema, grace_period=50, confidence=1e-07, binary_split=False, stop_mem_management=False) for i in range(n_experts)}\n",
    "\n",
    "router = RouterMLP()\n",
    "opt    = torch.optim.Adam(router.parameters(), lr=LR)\n",
    "nll    = nn.NLLLoss(reduction=\"mean\")\n",
    "\n",
    "pipeline_acc = metrics.Accuracy()\n",
    "running_loss = 0.0\n",
    "\n",
    "led_g_stream.restart()\n",
    "sea_g_stream.restart()\n",
    "#NUM_CLASSES = 2\n",
    "router.train()\n",
    "micro_X, micro_y = [], []\n",
    "\n",
    "for t in range(TOTAL_SAMPLES):\n",
    "    # 3-A  Embed sample\n",
    "\n",
    "    instance = led_g_stream.next_instance()\n",
    "    x_vec = instance.x\n",
    "    y_true = instance.y_index\n",
    "    x_t   = to_tensor(x_vec).unsqueeze(0)         # 1Ã—24\n",
    "\n",
    "    # 3-B  Router forward\n",
    "    logits  = router(x_t) # 1Ã—n_experts\n",
    "    #tau0, tau_min, decay_steps = 2.0, 0.7, 80_000   \n",
    "    #tau = max(tau_min, tau0 * (1 - t / decay_steps)) # linear-cosine also works\n",
    "    #weights = torch.softmax(logits / tau, dim=1)     # replaces previous softmax                       \n",
    "    weights = torch.softmax(logits, dim=1)        # 1Ã—n_experts\n",
    "\n",
    "    # 3-C  Gather expertsâ€™ probability vectors\n",
    "    exp_probs = []\n",
    "    for e in experts.values():\n",
    "        p_list = e.predict_proba(instance) or [1/NUM_CLASSES for c in CLASSES]\n",
    "        if p_list is None:                            # brand-new leaf\n",
    "            padded_p_list = [1 / NUM_CLASSES] * NUM_CLASSES  # uniform prior\n",
    "        elif len(p_list) < NUM_CLASSES:               # seen some classes\n",
    "            \n",
    "            padded_p_list = list(p_list) + [0.0] * (NUM_CLASSES - len(list(p_list)))\n",
    "        else:                                      # already full length\n",
    "            padded_p_list = list(p_list)\n",
    "        exp_probs.append(padded_p_list)\n",
    "    exp_probs = torch.tensor(exp_probs)           # n_experts Ã— C\n",
    "\n",
    "    mix_prob = torch.mm(weights, exp_probs) + 1e-9\n",
    "    log_mix  = (mix_prob / mix_prob.sum()).log()  # 1Ã—C log-probs\n",
    "\n",
    "    # 3-D  Accumulate mini-batch for router update\n",
    "    micro_X.append(log_mix)\n",
    "    micro_y.append(y_true)\n",
    "    if len(micro_X) == BATCH:\n",
    "        batch_X = torch.cat(micro_X, dim=0)       # BÃ—C\n",
    "        batch_y = torch.tensor(micro_y)\n",
    "        loss = nll(batch_X, batch_y)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        running_loss += loss.item() * BATCH\n",
    "        micro_X.clear(); micro_y.clear()\n",
    "\n",
    "    # 3-E  Top-K expert updates\n",
    "    with torch.no_grad():\n",
    "        topk_ids = torch.topk(weights, k=TOP_K, dim=1).indices.squeeze(0)\n",
    "    for eid in topk_ids.tolist():\n",
    "        experts[eid].train(instance)\n",
    "\n",
    "    # 3-F  Running metrics\n",
    "    y_hat = CLASSES[int(torch.argmax(mix_prob))]\n",
    "    pipeline_acc.update(y_true, y_hat)\n",
    "\n",
    "    if t % PRINT_EVERY == 0:\n",
    "        avg_ce = running_loss / max(1, (t // BATCH))\n",
    "        print(f\"[{t:,} samples]  router CE: {avg_ce:.4f}   \"\n",
    "              f\"pipeline acc: {pipeline_acc.get():.4f}\")\n",
    "        running_loss = 0.0\n",
    "\n",
    "print(\"ðŸ train-window accuracy:\", pipeline_acc.get())\n",
    "\n",
    "\"\"\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4.  Hold-out evaluation  (last 10 %)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "router.eval()\n",
    "hold_acc = metrics.Accuracy()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_dict, y_true in hold_stream:\n",
    "        x_vec = d2v(x_dict)\n",
    "        logits  = router(to_tensor(x_vec).unsqueeze(0))\n",
    "        weights = torch.softmax(logits, dim=1)\n",
    "        exp_probs = []\n",
    "        for e in experts.values():\n",
    "            pdict = e.predict_proba_one(x_dict) or {c: 1/NUM_CLASSES for c in CLASSES}\n",
    "            exp_probs.append([pdict.get(c, 0.0) for c in CLASSES])\n",
    "        exp_probs = torch.tensor(exp_probs)\n",
    "        mix_prob  = torch.mm(weights, exp_probs)\n",
    "        y_hat     = CLASSES[int(torch.argmax(mix_prob))]\n",
    "        hold_acc.update(y_true, y_hat)\n",
    "\n",
    "print(\"ðŸ hold-out (10 %) accuracy:\", hold_acc.get())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "d9a8d38a-46be-4b0e-9a43-076be3cde2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 samples]  PipeAcc = 1.0000\n",
      "[10,000 samples]  PipeAcc = 0.7602\n",
      "[20,000 samples]  PipeAcc = 0.7550\n",
      "[30,000 samples]  PipeAcc = 0.7515\n",
      "[40,000 samples]  PipeAcc = 0.7495\n",
      "[50,000 samples]  PipeAcc = 0.7499\n",
      "[60,000 samples]  PipeAcc = 0.7488\n",
      "[70,000 samples]  PipeAcc = 0.7481\n",
      "[80,000 samples]  PipeAcc = 0.7477\n",
      "[90,000 samples]  PipeAcc = 0.7478\n",
      "[100,000 samples]  PipeAcc = 0.7483\n",
      "[110,000 samples]  PipeAcc = 0.7479\n",
      "[120,000 samples]  PipeAcc = 0.7485\n",
      "[130,000 samples]  PipeAcc = 0.7481\n",
      "[140,000 samples]  PipeAcc = 0.7474\n",
      "[150,000 samples]  PipeAcc = 0.7481\n",
      "[160,000 samples]  PipeAcc = 0.7479\n",
      "[170,000 samples]  PipeAcc = 0.7478\n",
      "[180,000 samples]  PipeAcc = 0.7475\n",
      "[190,000 samples]  PipeAcc = 0.7470\n",
      "[200,000 samples]  PipeAcc = 0.7457\n",
      "[210,000 samples]  PipeAcc = 0.7435\n",
      "[220,000 samples]  PipeAcc = 0.7402\n",
      "[230,000 samples]  PipeAcc = 0.7361\n",
      "[240,000 samples]  PipeAcc = 0.7319\n",
      "[250,000 samples]  PipeAcc = 0.7296\n",
      "[260,000 samples]  PipeAcc = 0.7292\n",
      "[270,000 samples]  PipeAcc = 0.7297\n",
      "[280,000 samples]  PipeAcc = 0.7301\n",
      "[290,000 samples]  PipeAcc = 0.7303\n",
      "[300,000 samples]  PipeAcc = 0.7310\n",
      "[310,000 samples]  PipeAcc = 0.7314\n",
      "[320,000 samples]  PipeAcc = 0.7319\n",
      "[330,000 samples]  PipeAcc = 0.7323\n",
      "[340,000 samples]  PipeAcc = 0.7327\n",
      "[350,000 samples]  PipeAcc = 0.7330\n",
      "[360,000 samples]  PipeAcc = 0.7335\n",
      "[370,000 samples]  PipeAcc = 0.7339\n",
      "[380,000 samples]  PipeAcc = 0.7342\n",
      "[390,000 samples]  PipeAcc = 0.7344\n",
      "[400,000 samples]  PipeAcc = 0.7347\n",
      "[410,000 samples]  PipeAcc = 0.7350\n",
      "[420,000 samples]  PipeAcc = 0.7350\n",
      "[430,000 samples]  PipeAcc = 0.7350\n",
      "[440,000 samples]  PipeAcc = 0.7351\n",
      "[450,000 samples]  PipeAcc = 0.7349\n",
      "[460,000 samples]  PipeAcc = 0.7340\n",
      "[470,000 samples]  PipeAcc = 0.7326\n",
      "[480,000 samples]  PipeAcc = 0.7306\n",
      "[490,000 samples]  PipeAcc = 0.7288\n",
      "[500,000 samples]  PipeAcc = 0.7272\n",
      "[510,000 samples]  PipeAcc = 0.7267\n",
      "[520,000 samples]  PipeAcc = 0.7269\n",
      "[530,000 samples]  PipeAcc = 0.7271\n",
      "[540,000 samples]  PipeAcc = 0.7273\n",
      "[550,000 samples]  PipeAcc = 0.7277\n",
      "[560,000 samples]  PipeAcc = 0.7279\n",
      "[570,000 samples]  PipeAcc = 0.7282\n",
      "[580,000 samples]  PipeAcc = 0.7284\n",
      "[590,000 samples]  PipeAcc = 0.7288\n",
      "[600,000 samples]  PipeAcc = 0.7289\n",
      "[610,000 samples]  PipeAcc = 0.7292\n",
      "[620,000 samples]  PipeAcc = 0.7295\n",
      "[630,000 samples]  PipeAcc = 0.7297\n",
      "[640,000 samples]  PipeAcc = 0.7299\n",
      "[650,000 samples]  PipeAcc = 0.7301\n",
      "[660,000 samples]  PipeAcc = 0.7304\n",
      "[670,000 samples]  PipeAcc = 0.7305\n",
      "[680,000 samples]  PipeAcc = 0.7306\n",
      "[690,000 samples]  PipeAcc = 0.7308\n",
      "[700,000 samples]  PipeAcc = 0.7309\n",
      "[710,000 samples]  PipeAcc = 0.7307\n",
      "[720,000 samples]  PipeAcc = 0.7305\n",
      "[730,000 samples]  PipeAcc = 0.7303\n",
      "[740,000 samples]  PipeAcc = 0.7301\n",
      "[750,000 samples]  PipeAcc = 0.7297\n",
      "[760,000 samples]  PipeAcc = 0.7296\n",
      "[770,000 samples]  PipeAcc = 0.7296\n",
      "[780,000 samples]  PipeAcc = 0.7295\n",
      "[790,000 samples]  PipeAcc = 0.7296\n",
      "[800,000 samples]  PipeAcc = 0.7297\n",
      "[810,000 samples]  PipeAcc = 0.7298\n",
      "[820,000 samples]  PipeAcc = 0.7300\n",
      "[830,000 samples]  PipeAcc = 0.7301\n",
      "[840,000 samples]  PipeAcc = 0.7303\n",
      "[850,000 samples]  PipeAcc = 0.7304\n",
      "[860,000 samples]  PipeAcc = 0.7306\n",
      "[870,000 samples]  PipeAcc = 0.7308\n",
      "[880,000 samples]  PipeAcc = 0.7310\n",
      "[890,000 samples]  PipeAcc = 0.7311\n",
      "[900,000 samples]  PipeAcc = 0.7313\n",
      "[910,000 samples]  PipeAcc = 0.7315\n",
      "[920,000 samples]  PipeAcc = 0.7316\n",
      "[930,000 samples]  PipeAcc = 0.7317\n",
      "[940,000 samples]  PipeAcc = 0.7319\n",
      "[950,000 samples]  PipeAcc = 0.7320\n",
      "[960,000 samples]  PipeAcc = 0.7322\n",
      "[970,000 samples]  PipeAcc = 0.7323\n",
      "[980,000 samples]  PipeAcc = 0.7325\n",
      "[990,000 samples]  PipeAcc = 0.7326\n",
      "ðŸ Final pipeline accuracy: 0.732699\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3.  Online joint-training loop (BCE loss version)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "experts        = {i: HoeffdingTree(schema=schema, grace_period=50, confidence=1e-07, binary_split=False, stop_mem_management=False)\n",
    "                  for i in range(n_experts)}\n",
    "expert_metrics = {i: metrics.Accuracy() for i in range(n_experts)}\n",
    "\n",
    "router        = RouterMLP()\n",
    "opt_router    = torch.optim.Adam(router.parameters(), lr=LR)\n",
    "bce_loss      = nn.BCEWithLogitsLoss()\n",
    "pipeline_acc  = metrics.Accuracy()\n",
    "running_loss = 0.0\n",
    "\n",
    "# Buffer is not needed for KL, but we keep it if you want to inspect routerâ€weights\n",
    "router_buffer = deque(maxlen=10_000)\n",
    "\n",
    "led_g_stream.restart()\n",
    "router.train()\n",
    "micro_logits, micro_multi = [], []\n",
    "\n",
    "for t in range(TOTAL_SAMPLES):\n",
    "    instance = led_g_stream.next_instance()\n",
    "    x_list, y_true = instance.x, instance.y_index\n",
    "    x_t = to_tensor(x_list).unsqueeze(0)   # [1Ã—24]\n",
    "\n",
    "    # 4.1 â€“ Router forward (raw logits, no temperature)\n",
    "    logits  = router(x_t)                   # [1Ã—n_experts]\n",
    "    weights = torch.softmax(logits, dim=1)  # [1Ã—n_experts]\n",
    "    router_buffer.append(weights.squeeze(0))\n",
    "\n",
    "    # 4.2 â€“ Gather expert probabilities\n",
    "    exp_probs = []\n",
    "    for eid in range(n_experts):\n",
    "        p_list = experts[eid].predict_proba(instance)\n",
    "        if p_list is None:\n",
    "            padded = [1.0 / NUM_CLASSES] * NUM_CLASSES\n",
    "        else:\n",
    "            padded = list(p_list)\n",
    "            if len(padded) < NUM_CLASSES:\n",
    "                padded = padded + [0.0] * (NUM_CLASSES - len(padded))\n",
    "        exp_probs.append(padded)\n",
    "    exp_probs = torch.tensor(np.stack(exp_probs, axis=0), dtype=torch.float32)  # [n_experts Ã— C]\n",
    "\n",
    "    # 4.3 â€“ Build multi-hot â€œwhich experts predict y_true correctly?â€\n",
    "    correct_mask = torch.zeros(n_experts, dtype=torch.float32)\n",
    "    for eid in range(n_experts):\n",
    "        p_list = exp_probs[eid].numpy().tolist()\n",
    "        pred_cls = int(np.argmax(p_list))\n",
    "        if pred_cls == y_true:\n",
    "            correct_mask[eid] = 1.0\n",
    "    if correct_mask.sum() == 0.0:\n",
    "        # fallback: pick expert with highest P(y_true)\n",
    "        best_e = int(torch.argmax(exp_probs[:, y_true]).item())\n",
    "        correct_mask[best_e] = 1.0\n",
    "\n",
    "    # 4.4 â€“ Accumulate minibatch for BCE update\n",
    "    micro_logits.append(logits)                    # [1Ã—n_experts]\n",
    "    micro_multi.append(correct_mask.unsqueeze(0))  # [1Ã—n_experts]\n",
    "    if len(micro_logits) == BATCH:\n",
    "        batch_logits = torch.cat(micro_logits, dim=0)  # [BÃ—n_experts]\n",
    "        batch_multi  = torch.cat(micro_multi, dim=0)   # [BÃ—n_experts]\n",
    "        loss_bce     = bce_loss(batch_logits, batch_multi)\n",
    "        opt_router.zero_grad()\n",
    "        loss_bce.backward()\n",
    "        opt_router.step()\n",
    "        micro_logits.clear()\n",
    "        micro_multi.clear()\n",
    "\n",
    "    # 4.5 â€“ Top-K=3 expert updates\n",
    "    topk_ids = torch.topk(weights, k=TOP_K, dim=1).indices.squeeze(0)\n",
    "    for eid in topk_ids.tolist():\n",
    "        experts[eid].train(instance)\n",
    "        p_list = experts[eid].predict_proba(instance)\n",
    "        if p_list is None:\n",
    "            pred_cls = -1\n",
    "        else:\n",
    "            pred_cls = int(np.argmax(p_list))\n",
    "        expert_metrics[eid].update(y_true, pred_cls)\n",
    "\n",
    "    # 4.6 â€“ Pipeline accuracy (argmax weight)\n",
    "    chosen_eid = int(torch.argmax(weights).item())\n",
    "    p_list = experts[chosen_eid].predict_proba(instance)\n",
    "    if p_list is None:\n",
    "        y_hat = -1\n",
    "    else:\n",
    "        y_hat = int(np.argmax(p_list))\n",
    "    pipeline_acc.update(y_true, y_hat)\n",
    "\n",
    "    # 4.7 â€“ Logging\n",
    "    if t % 10_000 == 0:\n",
    "        print(f\"[{t:,} samples]  PipeAcc = {pipeline_acc.get():.4f}\")\n",
    "\n",
    "print(\"ðŸ Final pipeline accuracy:\", pipeline_acc.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b7857-176e-4e3c-9e54-59c6bed8c29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899e718-202d-4263-84f6-5d3d963adebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moe_paper",
   "language": "python",
   "name": "moe_paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
