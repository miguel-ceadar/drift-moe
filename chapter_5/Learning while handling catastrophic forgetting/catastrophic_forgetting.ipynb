{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catastrophic forgetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from Original Pytorch github https://github.com/pytorch/examples/blob/main/mnist/main.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "    # Calculate and return the average loss and accuracy\n",
    "    average_loss = total_loss / len(train_loader.dataset)\n",
    "    accuracy = 100.*correct / len(train_loader.dataset)\n",
    "    return accuracy, average_loss\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return accuracy, test_loss\n",
    "\n",
    "def train_and_test(args, model, device, train_loader, test_loader, optimizer, scheduler, title = \"\"):\n",
    "    train_losses = []  # Store training losses\n",
    "    test_accuracies = []  # Store test accuracies\n",
    "    test_losses = [] \n",
    "    train_accuracies = [] \n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train_accuracy, train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test_accuracy, test_loss = test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "        \n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "            # Print training loss and test accuracy during each epoch\n",
    "        print(f'Epoch {epoch}/{args.epochs}: Training Loss: {train_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "        \n",
    "    # Plot loss and accuracy after all epochs\n",
    "    plot_loss_and_accuracy(train_accuracies, train_losses, test_accuracies, test_losses, title)\n",
    "\n",
    "def save_model_state_dict(model_state_dict, filename, overwrite=True):\n",
    "    models_folder = \"models\"\n",
    "    \n",
    "    # Check if the models folder exists, if not, create it\n",
    "    if not os.path.exists(models_folder):\n",
    "        os.makedirs(models_folder)\n",
    "\n",
    "    file_path = os.path.join(models_folder, filename)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(file_path) and not overwrite:\n",
    "        user_input = input(f\"The file '{filename}' already exists. Do you want to overwrite it? (yes/no): \")\n",
    "        if user_input.lower() != 'yes':\n",
    "            print(\"Model not saved.\")\n",
    "            return\n",
    "\n",
    "    # Save the model state dict\n",
    "    torch.save(model_state_dict, file_path)\n",
    "    print(f\"Model state dict saved to: {file_path}\")\n",
    "def plot_loss_and_accuracy(train_accuracies, train_losses, test_accuracies, test_losses, title = \"\"):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Plot Training Loss and Accuracy\n",
    "    ax1.plot(train_losses, label='Training Loss')\n",
    "    ax1.plot(train_accuracies, label='Training Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_title(f'{title} - training data')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot Testing Loss and Accuracy\n",
    "    ax2.plot(test_losses, label='Testing Loss')\n",
    "    ax2.plot(test_accuracies, label='Testing Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title(f'{title} - testing data')\n",
    "    ax2.legend()\n",
    "def plot_loss_and_accuracy_single(test_accuracies, test_losses, title = \"\"):\n",
    "    fig, (ax2) = plt.subplots(1, 1, figsize=(12, 4))\n",
    "    # Plot Testing Loss and Accuracy\n",
    "    ax2.plot(test_losses, label='Testing Loss')\n",
    "    ax2.plot(test_accuracies, label='Testing Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_title(f'{title} - testing data')\n",
    "    ax2.legend()\n",
    "\n",
    "    \n",
    "def get_data_loaders(dataset, train_size, test_size, train_kwargs, test_kwargs):\n",
    "    train_set, test_set = random_split(dataset, [train_size, test_size])\n",
    "    train_loader = DataLoader(train_set, **train_kwargs)\n",
    "    test_loader = DataLoader(test_set, **test_kwargs)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                        help='disables macOS GPU training')\n",
    "parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "                        help='quickly check a single pass')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "parser.add_argument('--test_reduced', action='store_true', default=True,\n",
    "                        help='For Saving the current Model')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args, unknown = parser.parse_known_args()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "    \n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if use_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "        device = torch.device(\"mps\")\n",
    "else:\n",
    "        device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/29236 (0%)]\tLoss: 1.620534\n",
      "Train Epoch: 1 [640/29236 (2%)]\tLoss: 0.485315\n",
      "Train Epoch: 1 [1280/29236 (4%)]\tLoss: 0.211582\n",
      "Train Epoch: 1 [1920/29236 (7%)]\tLoss: 0.129892\n",
      "Train Epoch: 1 [2560/29236 (9%)]\tLoss: 0.110819\n",
      "Train Epoch: 1 [3200/29236 (11%)]\tLoss: 0.086438\n",
      "Train Epoch: 1 [3840/29236 (13%)]\tLoss: 0.067602\n",
      "Train Epoch: 1 [4480/29236 (15%)]\tLoss: 0.050723\n",
      "Train Epoch: 1 [5120/29236 (18%)]\tLoss: 0.059049\n",
      "Train Epoch: 1 [5760/29236 (20%)]\tLoss: 0.041223\n",
      "Train Epoch: 1 [6400/29236 (22%)]\tLoss: 0.034861\n",
      "Train Epoch: 1 [7040/29236 (24%)]\tLoss: 0.006323\n",
      "Train Epoch: 1 [7680/29236 (26%)]\tLoss: 0.009950\n",
      "Train Epoch: 1 [8320/29236 (28%)]\tLoss: 0.092527\n",
      "Train Epoch: 1 [8960/29236 (31%)]\tLoss: 0.057528\n",
      "Train Epoch: 1 [9600/29236 (33%)]\tLoss: 0.086590\n",
      "Train Epoch: 1 [10240/29236 (35%)]\tLoss: 0.022528\n",
      "Train Epoch: 1 [10880/29236 (37%)]\tLoss: 0.020158\n",
      "Train Epoch: 1 [11520/29236 (39%)]\tLoss: 0.003577\n",
      "Train Epoch: 1 [12160/29236 (42%)]\tLoss: 0.181705\n",
      "Train Epoch: 1 [12800/29236 (44%)]\tLoss: 0.015013\n",
      "Train Epoch: 1 [13440/29236 (46%)]\tLoss: 0.031992\n",
      "Train Epoch: 1 [14080/29236 (48%)]\tLoss: 0.012433\n",
      "Train Epoch: 1 [14720/29236 (50%)]\tLoss: 0.024289\n",
      "Train Epoch: 1 [15360/29236 (53%)]\tLoss: 0.075202\n",
      "Train Epoch: 1 [16000/29236 (55%)]\tLoss: 0.034498\n",
      "Train Epoch: 1 [16640/29236 (57%)]\tLoss: 0.006281\n",
      "Train Epoch: 1 [17280/29236 (59%)]\tLoss: 0.011108\n",
      "Train Epoch: 1 [17920/29236 (61%)]\tLoss: 0.138573\n",
      "Train Epoch: 1 [18560/29236 (63%)]\tLoss: 0.014602\n",
      "Train Epoch: 1 [19200/29236 (66%)]\tLoss: 0.049130\n",
      "Train Epoch: 1 [19840/29236 (68%)]\tLoss: 0.013690\n",
      "Train Epoch: 1 [20480/29236 (70%)]\tLoss: 0.040920\n",
      "Train Epoch: 1 [21120/29236 (72%)]\tLoss: 0.008731\n",
      "Train Epoch: 1 [21760/29236 (74%)]\tLoss: 0.010960\n",
      "Train Epoch: 1 [22400/29236 (77%)]\tLoss: 0.056233\n",
      "Train Epoch: 1 [23040/29236 (79%)]\tLoss: 0.014974\n",
      "Train Epoch: 1 [23680/29236 (81%)]\tLoss: 0.030779\n",
      "Train Epoch: 1 [24320/29236 (83%)]\tLoss: 0.039224\n",
      "Train Epoch: 1 [24960/29236 (85%)]\tLoss: 0.026558\n",
      "Train Epoch: 1 [25600/29236 (88%)]\tLoss: 0.016217\n",
      "Train Epoch: 1 [26240/29236 (90%)]\tLoss: 0.124913\n",
      "Train Epoch: 1 [26880/29236 (92%)]\tLoss: 0.000486\n",
      "Train Epoch: 1 [27520/29236 (94%)]\tLoss: 0.009545\n",
      "Train Epoch: 1 [28160/29236 (96%)]\tLoss: 0.097667\n",
      "Train Epoch: 1 [28800/29236 (98%)]\tLoss: 0.003728\n",
      "\n",
      "Test set: Average loss: 0.0277, Accuracy: 6008/7309 (82%)\n",
      "\n",
      "Epoch 1/14: Training Loss: 0.0014, Test Accuracy: 82.20%\n",
      "Train Epoch: 2 [0/29236 (0%)]\tLoss: 0.003326\n",
      "Train Epoch: 2 [640/29236 (2%)]\tLoss: 0.031439\n",
      "Train Epoch: 2 [1280/29236 (4%)]\tLoss: 0.008916\n",
      "Train Epoch: 2 [1920/29236 (7%)]\tLoss: 0.013295\n",
      "Train Epoch: 2 [2560/29236 (9%)]\tLoss: 0.031552\n",
      "Train Epoch: 2 [3200/29236 (11%)]\tLoss: 0.021638\n",
      "Train Epoch: 2 [3840/29236 (13%)]\tLoss: 0.059642\n",
      "Train Epoch: 2 [4480/29236 (15%)]\tLoss: 0.070131\n",
      "Train Epoch: 2 [5120/29236 (18%)]\tLoss: 0.073076\n",
      "Train Epoch: 2 [5760/29236 (20%)]\tLoss: 0.003954\n",
      "Train Epoch: 2 [6400/29236 (22%)]\tLoss: 0.008203\n",
      "Train Epoch: 2 [7040/29236 (24%)]\tLoss: 0.018196\n",
      "Train Epoch: 2 [7680/29236 (26%)]\tLoss: 0.000708\n",
      "Train Epoch: 2 [8320/29236 (28%)]\tLoss: 0.025342\n",
      "Train Epoch: 2 [8960/29236 (31%)]\tLoss: 0.022591\n",
      "Train Epoch: 2 [9600/29236 (33%)]\tLoss: 0.045246\n",
      "Train Epoch: 2 [10240/29236 (35%)]\tLoss: 0.006068\n",
      "Train Epoch: 2 [10880/29236 (37%)]\tLoss: 0.013148\n",
      "Train Epoch: 2 [11520/29236 (39%)]\tLoss: 0.005388\n",
      "Train Epoch: 2 [12160/29236 (42%)]\tLoss: 0.017964\n",
      "Train Epoch: 2 [12800/29236 (44%)]\tLoss: 0.003226\n",
      "Train Epoch: 2 [13440/29236 (46%)]\tLoss: 0.019860\n",
      "Train Epoch: 2 [14080/29236 (48%)]\tLoss: 0.004860\n",
      "Train Epoch: 2 [14720/29236 (50%)]\tLoss: 0.036164\n",
      "Train Epoch: 2 [15360/29236 (53%)]\tLoss: 0.030763\n",
      "Train Epoch: 2 [16000/29236 (55%)]\tLoss: 0.023757\n",
      "Train Epoch: 2 [16640/29236 (57%)]\tLoss: 0.003169\n",
      "Train Epoch: 2 [17280/29236 (59%)]\tLoss: 0.000955\n",
      "Train Epoch: 2 [17920/29236 (61%)]\tLoss: 0.001237\n",
      "Train Epoch: 2 [18560/29236 (63%)]\tLoss: 0.004088\n",
      "Train Epoch: 2 [19200/29236 (66%)]\tLoss: 0.002148\n",
      "Train Epoch: 2 [19840/29236 (68%)]\tLoss: 0.004649\n",
      "Train Epoch: 2 [20480/29236 (70%)]\tLoss: 0.004169\n",
      "Train Epoch: 2 [21120/29236 (72%)]\tLoss: 0.032060\n",
      "Train Epoch: 2 [21760/29236 (74%)]\tLoss: 0.002737\n",
      "Train Epoch: 2 [22400/29236 (77%)]\tLoss: 0.157524\n",
      "Train Epoch: 2 [23040/29236 (79%)]\tLoss: 0.013014\n",
      "Train Epoch: 2 [23680/29236 (81%)]\tLoss: 0.010089\n",
      "Train Epoch: 2 [24320/29236 (83%)]\tLoss: 0.064959\n",
      "Train Epoch: 2 [24960/29236 (85%)]\tLoss: 0.005800\n",
      "Train Epoch: 2 [25600/29236 (88%)]\tLoss: 0.001019\n",
      "Train Epoch: 2 [26240/29236 (90%)]\tLoss: 0.005839\n",
      "Train Epoch: 2 [26880/29236 (92%)]\tLoss: 0.001396\n",
      "Train Epoch: 2 [27520/29236 (94%)]\tLoss: 0.002804\n",
      "Train Epoch: 2 [28160/29236 (96%)]\tLoss: 0.104817\n",
      "Train Epoch: 2 [28800/29236 (98%)]\tLoss: 0.000686\n",
      "\n",
      "Test set: Average loss: 0.0204, Accuracy: 6029/7309 (82%)\n",
      "\n",
      "Epoch 2/14: Training Loss: 0.0004, Test Accuracy: 82.49%\n",
      "Train Epoch: 3 [0/29236 (0%)]\tLoss: 0.003215\n",
      "Train Epoch: 3 [640/29236 (2%)]\tLoss: 0.001032\n",
      "Train Epoch: 3 [1280/29236 (4%)]\tLoss: 0.002061\n",
      "Train Epoch: 3 [1920/29236 (7%)]\tLoss: 0.046587\n",
      "Train Epoch: 3 [2560/29236 (9%)]\tLoss: 0.006690\n",
      "Train Epoch: 3 [3200/29236 (11%)]\tLoss: 0.009517\n",
      "Train Epoch: 3 [3840/29236 (13%)]\tLoss: 0.037610\n",
      "Train Epoch: 3 [4480/29236 (15%)]\tLoss: 0.001352\n",
      "Train Epoch: 3 [5120/29236 (18%)]\tLoss: 0.006007\n",
      "Train Epoch: 3 [5760/29236 (20%)]\tLoss: 0.031581\n",
      "Train Epoch: 3 [6400/29236 (22%)]\tLoss: 0.001443\n",
      "Train Epoch: 3 [7040/29236 (24%)]\tLoss: 0.000679\n",
      "Train Epoch: 3 [7680/29236 (26%)]\tLoss: 0.007158\n",
      "Train Epoch: 3 [8320/29236 (28%)]\tLoss: 0.026246\n",
      "Train Epoch: 3 [8960/29236 (31%)]\tLoss: 0.002094\n",
      "Train Epoch: 3 [9600/29236 (33%)]\tLoss: 0.013610\n",
      "Train Epoch: 3 [10240/29236 (35%)]\tLoss: 0.009456\n",
      "Train Epoch: 3 [10880/29236 (37%)]\tLoss: 0.040845\n",
      "Train Epoch: 3 [11520/29236 (39%)]\tLoss: 0.000782\n",
      "Train Epoch: 3 [12160/29236 (42%)]\tLoss: 0.009694\n",
      "Train Epoch: 3 [12800/29236 (44%)]\tLoss: 0.000901\n",
      "Train Epoch: 3 [13440/29236 (46%)]\tLoss: 0.013120\n",
      "Train Epoch: 3 [14080/29236 (48%)]\tLoss: 0.000348\n",
      "Train Epoch: 3 [14720/29236 (50%)]\tLoss: 0.053679\n",
      "Train Epoch: 3 [15360/29236 (53%)]\tLoss: 0.002496\n",
      "Train Epoch: 3 [16000/29236 (55%)]\tLoss: 0.063697\n",
      "Train Epoch: 3 [16640/29236 (57%)]\tLoss: 0.002044\n",
      "Train Epoch: 3 [17280/29236 (59%)]\tLoss: 0.002929\n",
      "Train Epoch: 3 [17920/29236 (61%)]\tLoss: 0.000355\n",
      "Train Epoch: 3 [18560/29236 (63%)]\tLoss: 0.001356\n",
      "Train Epoch: 3 [19200/29236 (66%)]\tLoss: 0.002278\n",
      "Train Epoch: 3 [19840/29236 (68%)]\tLoss: 0.002003\n",
      "Train Epoch: 3 [20480/29236 (70%)]\tLoss: 0.005432\n",
      "Train Epoch: 3 [21120/29236 (72%)]\tLoss: 0.002556\n",
      "Train Epoch: 3 [21760/29236 (74%)]\tLoss: 0.027729\n",
      "Train Epoch: 3 [22400/29236 (77%)]\tLoss: 0.006443\n",
      "Train Epoch: 3 [23040/29236 (79%)]\tLoss: 0.025813\n",
      "Train Epoch: 3 [23680/29236 (81%)]\tLoss: 0.050299\n",
      "Train Epoch: 3 [24320/29236 (83%)]\tLoss: 0.024071\n",
      "Train Epoch: 3 [24960/29236 (85%)]\tLoss: 0.006160\n",
      "Train Epoch: 3 [25600/29236 (88%)]\tLoss: 0.000270\n",
      "Train Epoch: 3 [26240/29236 (90%)]\tLoss: 0.000316\n",
      "Train Epoch: 3 [26880/29236 (92%)]\tLoss: 0.000125\n",
      "Train Epoch: 3 [27520/29236 (94%)]\tLoss: 0.002926\n",
      "Train Epoch: 3 [28160/29236 (96%)]\tLoss: 0.094532\n",
      "Train Epoch: 3 [28800/29236 (98%)]\tLoss: 0.000231\n",
      "\n",
      "Test set: Average loss: 0.0153, Accuracy: 6038/7309 (83%)\n",
      "\n",
      "Epoch 3/14: Training Loss: 0.0003, Test Accuracy: 82.61%\n",
      "Train Epoch: 4 [0/29236 (0%)]\tLoss: 0.004894\n",
      "Train Epoch: 4 [640/29236 (2%)]\tLoss: 0.011544\n",
      "Train Epoch: 4 [1280/29236 (4%)]\tLoss: 0.001304\n",
      "Train Epoch: 4 [1920/29236 (7%)]\tLoss: 0.006433\n",
      "Train Epoch: 4 [2560/29236 (9%)]\tLoss: 0.000217\n",
      "Train Epoch: 4 [3200/29236 (11%)]\tLoss: 0.030789\n",
      "Train Epoch: 4 [3840/29236 (13%)]\tLoss: 0.012684\n",
      "Train Epoch: 4 [4480/29236 (15%)]\tLoss: 0.000196\n",
      "Train Epoch: 4 [5120/29236 (18%)]\tLoss: 0.009723\n",
      "Train Epoch: 4 [5760/29236 (20%)]\tLoss: 0.001613\n",
      "Train Epoch: 4 [6400/29236 (22%)]\tLoss: 0.002462\n",
      "Train Epoch: 4 [7040/29236 (24%)]\tLoss: 0.000183\n",
      "Train Epoch: 4 [7680/29236 (26%)]\tLoss: 0.000252\n",
      "Train Epoch: 4 [8320/29236 (28%)]\tLoss: 0.001688\n",
      "Train Epoch: 4 [8960/29236 (31%)]\tLoss: 0.002438\n",
      "Train Epoch: 4 [9600/29236 (33%)]\tLoss: 0.083275\n",
      "Train Epoch: 4 [10240/29236 (35%)]\tLoss: 0.001180\n",
      "Train Epoch: 4 [10880/29236 (37%)]\tLoss: 0.030943\n",
      "Train Epoch: 4 [11520/29236 (39%)]\tLoss: 0.005250\n",
      "Train Epoch: 4 [12160/29236 (42%)]\tLoss: 0.002855\n",
      "Train Epoch: 4 [12800/29236 (44%)]\tLoss: 0.003788\n",
      "Train Epoch: 4 [13440/29236 (46%)]\tLoss: 0.005963\n",
      "Train Epoch: 4 [14080/29236 (48%)]\tLoss: 0.003289\n",
      "Train Epoch: 4 [14720/29236 (50%)]\tLoss: 0.007543\n",
      "Train Epoch: 4 [15360/29236 (53%)]\tLoss: 0.002442\n",
      "Train Epoch: 4 [16000/29236 (55%)]\tLoss: 0.004810\n",
      "Train Epoch: 4 [16640/29236 (57%)]\tLoss: 0.002338\n",
      "Train Epoch: 4 [17280/29236 (59%)]\tLoss: 0.001050\n",
      "Train Epoch: 4 [17920/29236 (61%)]\tLoss: 0.007645\n",
      "Train Epoch: 4 [18560/29236 (63%)]\tLoss: 0.030476\n",
      "Train Epoch: 4 [19200/29236 (66%)]\tLoss: 0.002054\n",
      "Train Epoch: 4 [19840/29236 (68%)]\tLoss: 0.000938\n",
      "Train Epoch: 4 [20480/29236 (70%)]\tLoss: 0.003260\n",
      "Train Epoch: 4 [21120/29236 (72%)]\tLoss: 0.004142\n",
      "Train Epoch: 4 [21760/29236 (74%)]\tLoss: 0.000989\n",
      "Train Epoch: 4 [22400/29236 (77%)]\tLoss: 0.000134\n",
      "Train Epoch: 4 [23040/29236 (79%)]\tLoss: 0.005302\n",
      "Train Epoch: 4 [23680/29236 (81%)]\tLoss: 0.018118\n",
      "Train Epoch: 4 [24320/29236 (83%)]\tLoss: 0.027888\n",
      "Train Epoch: 4 [24960/29236 (85%)]\tLoss: 0.000134\n",
      "Train Epoch: 4 [25600/29236 (88%)]\tLoss: 0.000088\n",
      "Train Epoch: 4 [26240/29236 (90%)]\tLoss: 0.000795\n",
      "Train Epoch: 4 [26880/29236 (92%)]\tLoss: 0.000046\n",
      "Train Epoch: 4 [27520/29236 (94%)]\tLoss: 0.000243\n",
      "Train Epoch: 4 [28160/29236 (96%)]\tLoss: 0.092905\n",
      "Train Epoch: 4 [28800/29236 (98%)]\tLoss: 0.000204\n",
      "\n",
      "Test set: Average loss: 0.0159, Accuracy: 6040/7309 (83%)\n",
      "\n",
      "Epoch 4/14: Training Loss: 0.0002, Test Accuracy: 82.64%\n",
      "Train Epoch: 5 [0/29236 (0%)]\tLoss: 0.000629\n",
      "Train Epoch: 5 [640/29236 (2%)]\tLoss: 0.000438\n",
      "Train Epoch: 5 [1280/29236 (4%)]\tLoss: 0.000045\n",
      "Train Epoch: 5 [1920/29236 (7%)]\tLoss: 0.026068\n",
      "Train Epoch: 5 [2560/29236 (9%)]\tLoss: 0.008684\n",
      "Train Epoch: 5 [3200/29236 (11%)]\tLoss: 0.000286\n",
      "Train Epoch: 5 [3840/29236 (13%)]\tLoss: 0.008714\n",
      "Train Epoch: 5 [4480/29236 (15%)]\tLoss: 0.000519\n",
      "Train Epoch: 5 [5120/29236 (18%)]\tLoss: 0.006810\n",
      "Train Epoch: 5 [5760/29236 (20%)]\tLoss: 0.001861\n",
      "Train Epoch: 5 [6400/29236 (22%)]\tLoss: 0.000664\n",
      "Train Epoch: 5 [7040/29236 (24%)]\tLoss: 0.010344\n",
      "Train Epoch: 5 [7680/29236 (26%)]\tLoss: 0.001137\n",
      "Train Epoch: 5 [8320/29236 (28%)]\tLoss: 0.003316\n",
      "Train Epoch: 5 [8960/29236 (31%)]\tLoss: 0.000471\n",
      "Train Epoch: 5 [9600/29236 (33%)]\tLoss: 0.094643\n",
      "Train Epoch: 5 [10240/29236 (35%)]\tLoss: 0.001307\n",
      "Train Epoch: 5 [10880/29236 (37%)]\tLoss: 0.001923\n",
      "Train Epoch: 5 [11520/29236 (39%)]\tLoss: 0.000177\n",
      "Train Epoch: 5 [12160/29236 (42%)]\tLoss: 0.022902\n",
      "Train Epoch: 5 [12800/29236 (44%)]\tLoss: 0.007075\n",
      "Train Epoch: 5 [13440/29236 (46%)]\tLoss: 0.000475\n",
      "Train Epoch: 5 [14080/29236 (48%)]\tLoss: 0.001372\n",
      "Train Epoch: 5 [14720/29236 (50%)]\tLoss: 0.004096\n",
      "Train Epoch: 5 [15360/29236 (53%)]\tLoss: 0.002393\n",
      "Train Epoch: 5 [16000/29236 (55%)]\tLoss: 0.014041\n",
      "Train Epoch: 5 [16640/29236 (57%)]\tLoss: 0.000764\n",
      "Train Epoch: 5 [17280/29236 (59%)]\tLoss: 0.000434\n",
      "Train Epoch: 5 [17920/29236 (61%)]\tLoss: 0.001242\n",
      "Train Epoch: 5 [18560/29236 (63%)]\tLoss: 0.003429\n",
      "Train Epoch: 5 [19200/29236 (66%)]\tLoss: 0.002194\n",
      "Train Epoch: 5 [19840/29236 (68%)]\tLoss: 0.000137\n",
      "Train Epoch: 5 [20480/29236 (70%)]\tLoss: 0.004026\n",
      "Train Epoch: 5 [21120/29236 (72%)]\tLoss: 0.000715\n",
      "Train Epoch: 5 [21760/29236 (74%)]\tLoss: 0.010544\n",
      "Train Epoch: 5 [22400/29236 (77%)]\tLoss: 0.000107\n",
      "Train Epoch: 5 [23040/29236 (79%)]\tLoss: 0.022758\n",
      "Train Epoch: 5 [23680/29236 (81%)]\tLoss: 0.000577\n",
      "Train Epoch: 5 [24320/29236 (83%)]\tLoss: 0.010157\n",
      "Train Epoch: 5 [24960/29236 (85%)]\tLoss: 0.001277\n",
      "Train Epoch: 5 [25600/29236 (88%)]\tLoss: 0.000181\n",
      "Train Epoch: 5 [26240/29236 (90%)]\tLoss: 0.004740\n",
      "Train Epoch: 5 [26880/29236 (92%)]\tLoss: 0.000103\n",
      "Train Epoch: 5 [27520/29236 (94%)]\tLoss: 0.000174\n",
      "Train Epoch: 5 [28160/29236 (96%)]\tLoss: 0.087623\n",
      "Train Epoch: 5 [28800/29236 (98%)]\tLoss: 0.000388\n",
      "\n",
      "Test set: Average loss: 0.0146, Accuracy: 6043/7309 (83%)\n",
      "\n",
      "Epoch 5/14: Training Loss: 0.0002, Test Accuracy: 82.68%\n",
      "Train Epoch: 6 [0/29236 (0%)]\tLoss: 0.000403\n",
      "Train Epoch: 6 [640/29236 (2%)]\tLoss: 0.028265\n",
      "Train Epoch: 6 [1280/29236 (4%)]\tLoss: 0.001591\n",
      "Train Epoch: 6 [1920/29236 (7%)]\tLoss: 0.000163\n",
      "Train Epoch: 6 [2560/29236 (9%)]\tLoss: 0.001372\n",
      "Train Epoch: 6 [3200/29236 (11%)]\tLoss: 0.004513\n",
      "Train Epoch: 6 [3840/29236 (13%)]\tLoss: 0.017990\n",
      "Train Epoch: 6 [4480/29236 (15%)]\tLoss: 0.000233\n",
      "Train Epoch: 6 [5120/29236 (18%)]\tLoss: 0.018423\n",
      "Train Epoch: 6 [5760/29236 (20%)]\tLoss: 0.000923\n",
      "Train Epoch: 6 [6400/29236 (22%)]\tLoss: 0.000206\n",
      "Train Epoch: 6 [7040/29236 (24%)]\tLoss: 0.000222\n",
      "Train Epoch: 6 [7680/29236 (26%)]\tLoss: 0.000799\n",
      "Train Epoch: 6 [8320/29236 (28%)]\tLoss: 0.058804\n",
      "Train Epoch: 6 [8960/29236 (31%)]\tLoss: 0.004352\n",
      "Train Epoch: 6 [9600/29236 (33%)]\tLoss: 0.022017\n",
      "Train Epoch: 6 [10240/29236 (35%)]\tLoss: 0.000153\n",
      "Train Epoch: 6 [10880/29236 (37%)]\tLoss: 0.008647\n",
      "Train Epoch: 6 [11520/29236 (39%)]\tLoss: 0.000757\n",
      "Train Epoch: 6 [12160/29236 (42%)]\tLoss: 0.002008\n",
      "Train Epoch: 6 [12800/29236 (44%)]\tLoss: 0.000622\n",
      "Train Epoch: 6 [13440/29236 (46%)]\tLoss: 0.011740\n",
      "Train Epoch: 6 [14080/29236 (48%)]\tLoss: 0.000138\n",
      "Train Epoch: 6 [14720/29236 (50%)]\tLoss: 0.003747\n",
      "Train Epoch: 6 [15360/29236 (53%)]\tLoss: 0.004583\n",
      "Train Epoch: 6 [16000/29236 (55%)]\tLoss: 0.003352\n",
      "Train Epoch: 6 [16640/29236 (57%)]\tLoss: 0.000702\n",
      "Train Epoch: 6 [17280/29236 (59%)]\tLoss: 0.005709\n",
      "Train Epoch: 6 [17920/29236 (61%)]\tLoss: 0.000962\n",
      "Train Epoch: 6 [18560/29236 (63%)]\tLoss: 0.003175\n",
      "Train Epoch: 6 [19200/29236 (66%)]\tLoss: 0.001629\n",
      "Train Epoch: 6 [19840/29236 (68%)]\tLoss: 0.008405\n",
      "Train Epoch: 6 [20480/29236 (70%)]\tLoss: 0.000300\n",
      "Train Epoch: 6 [21120/29236 (72%)]\tLoss: 0.001832\n",
      "Train Epoch: 6 [21760/29236 (74%)]\tLoss: 0.000196\n",
      "Train Epoch: 6 [22400/29236 (77%)]\tLoss: 0.001751\n",
      "Train Epoch: 6 [23040/29236 (79%)]\tLoss: 0.009511\n",
      "Train Epoch: 6 [23680/29236 (81%)]\tLoss: 0.021327\n",
      "Train Epoch: 6 [24320/29236 (83%)]\tLoss: 0.030532\n",
      "Train Epoch: 6 [24960/29236 (85%)]\tLoss: 0.000086\n",
      "Train Epoch: 6 [25600/29236 (88%)]\tLoss: 0.000882\n",
      "Train Epoch: 6 [26240/29236 (90%)]\tLoss: 0.000222\n",
      "Train Epoch: 6 [26880/29236 (92%)]\tLoss: 0.000072\n",
      "Train Epoch: 6 [27520/29236 (94%)]\tLoss: 0.000247\n",
      "Train Epoch: 6 [28160/29236 (96%)]\tLoss: 0.028987\n",
      "Train Epoch: 6 [28800/29236 (98%)]\tLoss: 0.000447\n",
      "\n",
      "Test set: Average loss: 0.0139, Accuracy: 6046/7309 (83%)\n",
      "\n",
      "Epoch 6/14: Training Loss: 0.0001, Test Accuracy: 82.72%\n",
      "Train Epoch: 7 [0/29236 (0%)]\tLoss: 0.000557\n",
      "Train Epoch: 7 [640/29236 (2%)]\tLoss: 0.000224\n",
      "Train Epoch: 7 [1280/29236 (4%)]\tLoss: 0.000265\n",
      "Train Epoch: 7 [1920/29236 (7%)]\tLoss: 0.005205\n",
      "Train Epoch: 7 [2560/29236 (9%)]\tLoss: 0.000416\n",
      "Train Epoch: 7 [3200/29236 (11%)]\tLoss: 0.000396\n",
      "Train Epoch: 7 [3840/29236 (13%)]\tLoss: 0.018786\n",
      "Train Epoch: 7 [4480/29236 (15%)]\tLoss: 0.000276\n",
      "Train Epoch: 7 [5120/29236 (18%)]\tLoss: 0.102917\n",
      "Train Epoch: 7 [5760/29236 (20%)]\tLoss: 0.004703\n",
      "Train Epoch: 7 [6400/29236 (22%)]\tLoss: 0.005111\n",
      "Train Epoch: 7 [7040/29236 (24%)]\tLoss: 0.002737\n",
      "Train Epoch: 7 [7680/29236 (26%)]\tLoss: 0.000038\n",
      "Train Epoch: 7 [8320/29236 (28%)]\tLoss: 0.005803\n",
      "Train Epoch: 7 [8960/29236 (31%)]\tLoss: 0.000600\n",
      "Train Epoch: 7 [9600/29236 (33%)]\tLoss: 0.019230\n",
      "Train Epoch: 7 [10240/29236 (35%)]\tLoss: 0.000280\n",
      "Train Epoch: 7 [10880/29236 (37%)]\tLoss: 0.003138\n",
      "Train Epoch: 7 [11520/29236 (39%)]\tLoss: 0.000374\n",
      "Train Epoch: 7 [12160/29236 (42%)]\tLoss: 0.000270\n",
      "Train Epoch: 7 [12800/29236 (44%)]\tLoss: 0.000255\n",
      "Train Epoch: 7 [13440/29236 (46%)]\tLoss: 0.001215\n",
      "Train Epoch: 7 [14080/29236 (48%)]\tLoss: 0.000112\n",
      "Train Epoch: 7 [14720/29236 (50%)]\tLoss: 0.003513\n",
      "Train Epoch: 7 [15360/29236 (53%)]\tLoss: 0.009887\n",
      "Train Epoch: 7 [16000/29236 (55%)]\tLoss: 0.000343\n",
      "Train Epoch: 7 [16640/29236 (57%)]\tLoss: 0.001453\n",
      "Train Epoch: 7 [17280/29236 (59%)]\tLoss: 0.000956\n",
      "Train Epoch: 7 [17920/29236 (61%)]\tLoss: 0.002609\n",
      "Train Epoch: 7 [18560/29236 (63%)]\tLoss: 0.001507\n",
      "Train Epoch: 7 [19200/29236 (66%)]\tLoss: 0.000362\n",
      "Train Epoch: 7 [19840/29236 (68%)]\tLoss: 0.000205\n",
      "Train Epoch: 7 [20480/29236 (70%)]\tLoss: 0.002034\n",
      "Train Epoch: 7 [21120/29236 (72%)]\tLoss: 0.001180\n",
      "Train Epoch: 7 [21760/29236 (74%)]\tLoss: 0.005815\n",
      "Train Epoch: 7 [22400/29236 (77%)]\tLoss: 0.039234\n",
      "Train Epoch: 7 [23040/29236 (79%)]\tLoss: 0.002422\n",
      "Train Epoch: 7 [23680/29236 (81%)]\tLoss: 0.000158\n",
      "Train Epoch: 7 [24320/29236 (83%)]\tLoss: 0.008669\n",
      "Train Epoch: 7 [24960/29236 (85%)]\tLoss: 0.000280\n",
      "Train Epoch: 7 [25600/29236 (88%)]\tLoss: 0.003844\n",
      "Train Epoch: 7 [26240/29236 (90%)]\tLoss: 0.000089\n",
      "Train Epoch: 7 [26880/29236 (92%)]\tLoss: 0.000020\n",
      "Train Epoch: 7 [27520/29236 (94%)]\tLoss: 0.003181\n",
      "Train Epoch: 7 [28160/29236 (96%)]\tLoss: 0.041890\n",
      "Train Epoch: 7 [28800/29236 (98%)]\tLoss: 0.000042\n",
      "\n",
      "Test set: Average loss: 0.0144, Accuracy: 6044/7309 (83%)\n",
      "\n",
      "Epoch 7/14: Training Loss: 0.0001, Test Accuracy: 82.69%\n",
      "Train Epoch: 8 [0/29236 (0%)]\tLoss: 0.003486\n",
      "Train Epoch: 8 [640/29236 (2%)]\tLoss: 0.000889\n",
      "Train Epoch: 8 [1280/29236 (4%)]\tLoss: 0.000214\n",
      "Train Epoch: 8 [1920/29236 (7%)]\tLoss: 0.000320\n",
      "Train Epoch: 8 [2560/29236 (9%)]\tLoss: 0.004227\n",
      "Train Epoch: 8 [3200/29236 (11%)]\tLoss: 0.003825\n",
      "Train Epoch: 8 [3840/29236 (13%)]\tLoss: 0.004187\n",
      "Train Epoch: 8 [4480/29236 (15%)]\tLoss: 0.000157\n",
      "Train Epoch: 8 [5120/29236 (18%)]\tLoss: 0.012040\n",
      "Train Epoch: 8 [5760/29236 (20%)]\tLoss: 0.000467\n",
      "Train Epoch: 8 [6400/29236 (22%)]\tLoss: 0.001116\n",
      "Train Epoch: 8 [7040/29236 (24%)]\tLoss: 0.002799\n",
      "Train Epoch: 8 [7680/29236 (26%)]\tLoss: 0.000479\n",
      "Train Epoch: 8 [8320/29236 (28%)]\tLoss: 0.001800\n",
      "Train Epoch: 8 [8960/29236 (31%)]\tLoss: 0.007999\n",
      "Train Epoch: 8 [9600/29236 (33%)]\tLoss: 0.006735\n",
      "Train Epoch: 8 [10240/29236 (35%)]\tLoss: 0.000177\n",
      "Train Epoch: 8 [10880/29236 (37%)]\tLoss: 0.001582\n",
      "Train Epoch: 8 [11520/29236 (39%)]\tLoss: 0.000061\n",
      "Train Epoch: 8 [12160/29236 (42%)]\tLoss: 0.002405\n",
      "Train Epoch: 8 [12800/29236 (44%)]\tLoss: 0.001125\n",
      "Train Epoch: 8 [13440/29236 (46%)]\tLoss: 0.001308\n",
      "Train Epoch: 8 [14080/29236 (48%)]\tLoss: 0.000028\n",
      "Train Epoch: 8 [14720/29236 (50%)]\tLoss: 0.001646\n",
      "Train Epoch: 8 [15360/29236 (53%)]\tLoss: 0.004584\n",
      "Train Epoch: 8 [16000/29236 (55%)]\tLoss: 0.000206\n",
      "Train Epoch: 8 [16640/29236 (57%)]\tLoss: 0.006198\n",
      "Train Epoch: 8 [17280/29236 (59%)]\tLoss: 0.000261\n",
      "Train Epoch: 8 [17920/29236 (61%)]\tLoss: 0.001582\n",
      "Train Epoch: 8 [18560/29236 (63%)]\tLoss: 0.000411\n",
      "Train Epoch: 8 [19200/29236 (66%)]\tLoss: 0.000254\n",
      "Train Epoch: 8 [19840/29236 (68%)]\tLoss: 0.000342\n",
      "Train Epoch: 8 [20480/29236 (70%)]\tLoss: 0.000078\n",
      "Train Epoch: 8 [21120/29236 (72%)]\tLoss: 0.000756\n",
      "Train Epoch: 8 [21760/29236 (74%)]\tLoss: 0.000568\n",
      "Train Epoch: 8 [22400/29236 (77%)]\tLoss: 0.000093\n",
      "Train Epoch: 8 [23040/29236 (79%)]\tLoss: 0.000115\n",
      "Train Epoch: 8 [23680/29236 (81%)]\tLoss: 0.000780\n",
      "Train Epoch: 8 [24320/29236 (83%)]\tLoss: 0.018099\n",
      "Train Epoch: 8 [24960/29236 (85%)]\tLoss: 0.008355\n",
      "Train Epoch: 8 [25600/29236 (88%)]\tLoss: 0.000044\n",
      "Train Epoch: 8 [26240/29236 (90%)]\tLoss: 0.001244\n",
      "Train Epoch: 8 [26880/29236 (92%)]\tLoss: 0.000465\n",
      "Train Epoch: 8 [27520/29236 (94%)]\tLoss: 0.000211\n",
      "Train Epoch: 8 [28160/29236 (96%)]\tLoss: 0.103754\n",
      "Train Epoch: 8 [28800/29236 (98%)]\tLoss: 0.000021\n",
      "\n",
      "Test set: Average loss: 0.0146, Accuracy: 6044/7309 (83%)\n",
      "\n",
      "Epoch 8/14: Training Loss: 0.0001, Test Accuracy: 82.69%\n",
      "Train Epoch: 9 [0/29236 (0%)]\tLoss: 0.003624\n",
      "Train Epoch: 9 [640/29236 (2%)]\tLoss: 0.007816\n",
      "Train Epoch: 9 [1280/29236 (4%)]\tLoss: 0.000119\n",
      "Train Epoch: 9 [1920/29236 (7%)]\tLoss: 0.000674\n",
      "Train Epoch: 9 [2560/29236 (9%)]\tLoss: 0.009334\n",
      "Train Epoch: 9 [3200/29236 (11%)]\tLoss: 0.000029\n",
      "Train Epoch: 9 [3840/29236 (13%)]\tLoss: 0.004487\n",
      "Train Epoch: 9 [4480/29236 (15%)]\tLoss: 0.000528\n",
      "Train Epoch: 9 [5120/29236 (18%)]\tLoss: 0.000241\n",
      "Train Epoch: 9 [5760/29236 (20%)]\tLoss: 0.002257\n",
      "Train Epoch: 9 [6400/29236 (22%)]\tLoss: 0.001400\n",
      "Train Epoch: 9 [7040/29236 (24%)]\tLoss: 0.015271\n",
      "Train Epoch: 9 [7680/29236 (26%)]\tLoss: 0.000393\n",
      "Train Epoch: 9 [8320/29236 (28%)]\tLoss: 0.008001\n",
      "Train Epoch: 9 [8960/29236 (31%)]\tLoss: 0.002261\n",
      "Train Epoch: 9 [9600/29236 (33%)]\tLoss: 0.039271\n",
      "Train Epoch: 9 [10240/29236 (35%)]\tLoss: 0.000192\n",
      "Train Epoch: 9 [10880/29236 (37%)]\tLoss: 0.000320\n",
      "Train Epoch: 9 [11520/29236 (39%)]\tLoss: 0.000365\n",
      "Train Epoch: 9 [12160/29236 (42%)]\tLoss: 0.000335\n",
      "Train Epoch: 9 [12800/29236 (44%)]\tLoss: 0.000223\n",
      "Train Epoch: 9 [13440/29236 (46%)]\tLoss: 0.005111\n",
      "Train Epoch: 9 [14080/29236 (48%)]\tLoss: 0.000004\n",
      "Train Epoch: 9 [14720/29236 (50%)]\tLoss: 0.000469\n",
      "Train Epoch: 9 [15360/29236 (53%)]\tLoss: 0.000452\n",
      "Train Epoch: 9 [16000/29236 (55%)]\tLoss: 0.000305\n",
      "Train Epoch: 9 [16640/29236 (57%)]\tLoss: 0.000109\n",
      "Train Epoch: 9 [17280/29236 (59%)]\tLoss: 0.000039\n",
      "Train Epoch: 9 [17920/29236 (61%)]\tLoss: 0.013632\n",
      "Train Epoch: 9 [18560/29236 (63%)]\tLoss: 0.000297\n",
      "Train Epoch: 9 [19200/29236 (66%)]\tLoss: 0.003044\n",
      "Train Epoch: 9 [19840/29236 (68%)]\tLoss: 0.000137\n",
      "Train Epoch: 9 [20480/29236 (70%)]\tLoss: 0.002203\n",
      "Train Epoch: 9 [21120/29236 (72%)]\tLoss: 0.002617\n",
      "Train Epoch: 9 [21760/29236 (74%)]\tLoss: 0.000150\n",
      "Train Epoch: 9 [22400/29236 (77%)]\tLoss: 0.000269\n",
      "Train Epoch: 9 [23040/29236 (79%)]\tLoss: 0.001896\n",
      "Train Epoch: 9 [23680/29236 (81%)]\tLoss: 0.000260\n",
      "Train Epoch: 9 [24320/29236 (83%)]\tLoss: 0.010256\n",
      "Train Epoch: 9 [24960/29236 (85%)]\tLoss: 0.001984\n",
      "Train Epoch: 9 [25600/29236 (88%)]\tLoss: 0.000526\n",
      "Train Epoch: 9 [26240/29236 (90%)]\tLoss: 0.002684\n",
      "Train Epoch: 9 [26880/29236 (92%)]\tLoss: 0.000030\n",
      "Train Epoch: 9 [27520/29236 (94%)]\tLoss: 0.007089\n",
      "Train Epoch: 9 [28160/29236 (96%)]\tLoss: 0.036479\n",
      "Train Epoch: 9 [28800/29236 (98%)]\tLoss: 0.000239\n",
      "\n",
      "Test set: Average loss: 0.0154, Accuracy: 6043/7309 (83%)\n",
      "\n",
      "Epoch 9/14: Training Loss: 0.0001, Test Accuracy: 82.68%\n",
      "Train Epoch: 10 [0/29236 (0%)]\tLoss: 0.001989\n",
      "Train Epoch: 10 [640/29236 (2%)]\tLoss: 0.001436\n",
      "Train Epoch: 10 [1280/29236 (4%)]\tLoss: 0.000795\n",
      "Train Epoch: 10 [1920/29236 (7%)]\tLoss: 0.001280\n",
      "Train Epoch: 10 [2560/29236 (9%)]\tLoss: 0.000749\n",
      "Train Epoch: 10 [3200/29236 (11%)]\tLoss: 0.000671\n",
      "Train Epoch: 10 [3840/29236 (13%)]\tLoss: 0.001224\n",
      "Train Epoch: 10 [4480/29236 (15%)]\tLoss: 0.000047\n",
      "Train Epoch: 10 [5120/29236 (18%)]\tLoss: 0.000593\n",
      "Train Epoch: 10 [5760/29236 (20%)]\tLoss: 0.000458\n",
      "Train Epoch: 10 [6400/29236 (22%)]\tLoss: 0.000083\n",
      "Train Epoch: 10 [7040/29236 (24%)]\tLoss: 0.000561\n",
      "Train Epoch: 10 [7680/29236 (26%)]\tLoss: 0.000015\n",
      "Train Epoch: 10 [8320/29236 (28%)]\tLoss: 0.000698\n",
      "Train Epoch: 10 [8960/29236 (31%)]\tLoss: 0.002529\n",
      "Train Epoch: 10 [9600/29236 (33%)]\tLoss: 0.038602\n",
      "Train Epoch: 10 [10240/29236 (35%)]\tLoss: 0.000113\n",
      "Train Epoch: 10 [10880/29236 (37%)]\tLoss: 0.001504\n",
      "Train Epoch: 10 [11520/29236 (39%)]\tLoss: 0.000060\n",
      "Train Epoch: 10 [12160/29236 (42%)]\tLoss: 0.000262\n",
      "Train Epoch: 10 [12800/29236 (44%)]\tLoss: 0.000458\n",
      "Train Epoch: 10 [13440/29236 (46%)]\tLoss: 0.002661\n",
      "Train Epoch: 10 [14080/29236 (48%)]\tLoss: 0.000127\n",
      "Train Epoch: 10 [14720/29236 (50%)]\tLoss: 0.000846\n",
      "Train Epoch: 10 [15360/29236 (53%)]\tLoss: 0.005476\n",
      "Train Epoch: 10 [16000/29236 (55%)]\tLoss: 0.004969\n",
      "Train Epoch: 10 [16640/29236 (57%)]\tLoss: 0.000200\n",
      "Train Epoch: 10 [17280/29236 (59%)]\tLoss: 0.000416\n",
      "Train Epoch: 10 [17920/29236 (61%)]\tLoss: 0.002630\n",
      "Train Epoch: 10 [18560/29236 (63%)]\tLoss: 0.000631\n",
      "Train Epoch: 10 [19200/29236 (66%)]\tLoss: 0.000437\n",
      "Train Epoch: 10 [19840/29236 (68%)]\tLoss: 0.001066\n",
      "Train Epoch: 10 [20480/29236 (70%)]\tLoss: 0.000869\n",
      "Train Epoch: 10 [21120/29236 (72%)]\tLoss: 0.000179\n",
      "Train Epoch: 10 [21760/29236 (74%)]\tLoss: 0.001513\n",
      "Train Epoch: 10 [22400/29236 (77%)]\tLoss: 0.000125\n",
      "Train Epoch: 10 [23040/29236 (79%)]\tLoss: 0.007762\n",
      "Train Epoch: 10 [23680/29236 (81%)]\tLoss: 0.000486\n",
      "Train Epoch: 10 [24320/29236 (83%)]\tLoss: 0.014350\n",
      "Train Epoch: 10 [24960/29236 (85%)]\tLoss: 0.001812\n",
      "Train Epoch: 10 [25600/29236 (88%)]\tLoss: 0.001322\n",
      "Train Epoch: 10 [26240/29236 (90%)]\tLoss: 0.000396\n",
      "Train Epoch: 10 [26880/29236 (92%)]\tLoss: 0.000069\n",
      "Train Epoch: 10 [27520/29236 (94%)]\tLoss: 0.000107\n",
      "Train Epoch: 10 [28160/29236 (96%)]\tLoss: 0.031546\n",
      "Train Epoch: 10 [28800/29236 (98%)]\tLoss: 0.000210\n",
      "\n",
      "Test set: Average loss: 0.0148, Accuracy: 6043/7309 (83%)\n",
      "\n",
      "Epoch 10/14: Training Loss: 0.0001, Test Accuracy: 82.68%\n",
      "Train Epoch: 11 [0/29236 (0%)]\tLoss: 0.000167\n",
      "Train Epoch: 11 [640/29236 (2%)]\tLoss: 0.001803\n",
      "Train Epoch: 11 [1280/29236 (4%)]\tLoss: 0.001863\n",
      "Train Epoch: 11 [1920/29236 (7%)]\tLoss: 0.000405\n",
      "Train Epoch: 11 [2560/29236 (9%)]\tLoss: 0.000832\n",
      "Train Epoch: 11 [3200/29236 (11%)]\tLoss: 0.000436\n",
      "Train Epoch: 11 [3840/29236 (13%)]\tLoss: 0.007743\n",
      "Train Epoch: 11 [4480/29236 (15%)]\tLoss: 0.000860\n",
      "Train Epoch: 11 [5120/29236 (18%)]\tLoss: 0.003961\n",
      "Train Epoch: 11 [5760/29236 (20%)]\tLoss: 0.000625\n",
      "Train Epoch: 11 [6400/29236 (22%)]\tLoss: 0.000206\n",
      "Train Epoch: 11 [7040/29236 (24%)]\tLoss: 0.006419\n",
      "Train Epoch: 11 [7680/29236 (26%)]\tLoss: 0.000024\n",
      "Train Epoch: 11 [8320/29236 (28%)]\tLoss: 0.000718\n",
      "Train Epoch: 11 [8960/29236 (31%)]\tLoss: 0.007384\n",
      "Train Epoch: 11 [9600/29236 (33%)]\tLoss: 0.045008\n",
      "Train Epoch: 11 [10240/29236 (35%)]\tLoss: 0.000327\n",
      "Train Epoch: 11 [10880/29236 (37%)]\tLoss: 0.001902\n",
      "Train Epoch: 11 [11520/29236 (39%)]\tLoss: 0.000257\n",
      "Train Epoch: 11 [12160/29236 (42%)]\tLoss: 0.000294\n",
      "Train Epoch: 11 [12800/29236 (44%)]\tLoss: 0.001805\n",
      "Train Epoch: 11 [13440/29236 (46%)]\tLoss: 0.002025\n",
      "Train Epoch: 11 [14080/29236 (48%)]\tLoss: 0.001759\n",
      "Train Epoch: 11 [14720/29236 (50%)]\tLoss: 0.004685\n",
      "Train Epoch: 11 [15360/29236 (53%)]\tLoss: 0.000054\n",
      "Train Epoch: 11 [16000/29236 (55%)]\tLoss: 0.001392\n",
      "Train Epoch: 11 [16640/29236 (57%)]\tLoss: 0.000251\n",
      "Train Epoch: 11 [17280/29236 (59%)]\tLoss: 0.000235\n",
      "Train Epoch: 11 [17920/29236 (61%)]\tLoss: 0.015206\n",
      "Train Epoch: 11 [18560/29236 (63%)]\tLoss: 0.006944\n",
      "Train Epoch: 11 [19200/29236 (66%)]\tLoss: 0.000989\n",
      "Train Epoch: 11 [19840/29236 (68%)]\tLoss: 0.000031\n",
      "Train Epoch: 11 [20480/29236 (70%)]\tLoss: 0.000485\n",
      "Train Epoch: 11 [21120/29236 (72%)]\tLoss: 0.000344\n",
      "Train Epoch: 11 [21760/29236 (74%)]\tLoss: 0.003139\n",
      "Train Epoch: 11 [22400/29236 (77%)]\tLoss: 0.027480\n",
      "Train Epoch: 11 [23040/29236 (79%)]\tLoss: 0.000377\n",
      "Train Epoch: 11 [23680/29236 (81%)]\tLoss: 0.000209\n",
      "Train Epoch: 11 [24320/29236 (83%)]\tLoss: 0.002925\n",
      "Train Epoch: 11 [24960/29236 (85%)]\tLoss: 0.000266\n",
      "Train Epoch: 11 [25600/29236 (88%)]\tLoss: 0.000618\n",
      "Train Epoch: 11 [26240/29236 (90%)]\tLoss: 0.001962\n",
      "Train Epoch: 11 [26880/29236 (92%)]\tLoss: 0.000100\n",
      "Train Epoch: 11 [27520/29236 (94%)]\tLoss: 0.000212\n",
      "Train Epoch: 11 [28160/29236 (96%)]\tLoss: 0.026128\n",
      "Train Epoch: 11 [28800/29236 (98%)]\tLoss: 0.001226\n",
      "\n",
      "Test set: Average loss: 0.0151, Accuracy: 6044/7309 (83%)\n",
      "\n",
      "Epoch 11/14: Training Loss: 0.0001, Test Accuracy: 82.69%\n",
      "Train Epoch: 12 [0/29236 (0%)]\tLoss: 0.000006\n",
      "Train Epoch: 12 [640/29236 (2%)]\tLoss: 0.000073\n",
      "Train Epoch: 12 [1280/29236 (4%)]\tLoss: 0.000582\n",
      "Train Epoch: 12 [1920/29236 (7%)]\tLoss: 0.002427\n",
      "Train Epoch: 12 [2560/29236 (9%)]\tLoss: 0.007455\n",
      "Train Epoch: 12 [3200/29236 (11%)]\tLoss: 0.001139\n",
      "Train Epoch: 12 [3840/29236 (13%)]\tLoss: 0.021680\n",
      "Train Epoch: 12 [4480/29236 (15%)]\tLoss: 0.000056\n",
      "Train Epoch: 12 [5120/29236 (18%)]\tLoss: 0.005676\n",
      "Train Epoch: 12 [5760/29236 (20%)]\tLoss: 0.000116\n",
      "Train Epoch: 12 [6400/29236 (22%)]\tLoss: 0.001838\n",
      "Train Epoch: 12 [7040/29236 (24%)]\tLoss: 0.002438\n",
      "Train Epoch: 12 [7680/29236 (26%)]\tLoss: 0.000032\n",
      "Train Epoch: 12 [8320/29236 (28%)]\tLoss: 0.001289\n",
      "Train Epoch: 12 [8960/29236 (31%)]\tLoss: 0.001170\n",
      "Train Epoch: 12 [9600/29236 (33%)]\tLoss: 0.005960\n",
      "Train Epoch: 12 [10240/29236 (35%)]\tLoss: 0.000963\n",
      "Train Epoch: 12 [10880/29236 (37%)]\tLoss: 0.000240\n",
      "Train Epoch: 12 [11520/29236 (39%)]\tLoss: 0.000120\n",
      "Train Epoch: 12 [12160/29236 (42%)]\tLoss: 0.000192\n",
      "Train Epoch: 12 [12800/29236 (44%)]\tLoss: 0.000246\n",
      "Train Epoch: 12 [13440/29236 (46%)]\tLoss: 0.000109\n",
      "Train Epoch: 12 [14080/29236 (48%)]\tLoss: 0.000053\n",
      "Train Epoch: 12 [14720/29236 (50%)]\tLoss: 0.000524\n",
      "Train Epoch: 12 [15360/29236 (53%)]\tLoss: 0.000701\n",
      "Train Epoch: 12 [16000/29236 (55%)]\tLoss: 0.015755\n",
      "Train Epoch: 12 [16640/29236 (57%)]\tLoss: 0.003909\n",
      "Train Epoch: 12 [17280/29236 (59%)]\tLoss: 0.000673\n",
      "Train Epoch: 12 [17920/29236 (61%)]\tLoss: 0.001572\n",
      "Train Epoch: 12 [18560/29236 (63%)]\tLoss: 0.003146\n",
      "Train Epoch: 12 [19200/29236 (66%)]\tLoss: 0.000095\n",
      "Train Epoch: 12 [19840/29236 (68%)]\tLoss: 0.000124\n",
      "Train Epoch: 12 [20480/29236 (70%)]\tLoss: 0.001286\n",
      "Train Epoch: 12 [21120/29236 (72%)]\tLoss: 0.001994\n",
      "Train Epoch: 12 [21760/29236 (74%)]\tLoss: 0.000375\n",
      "Train Epoch: 12 [22400/29236 (77%)]\tLoss: 0.000208\n",
      "Train Epoch: 12 [23040/29236 (79%)]\tLoss: 0.001286\n",
      "Train Epoch: 12 [23680/29236 (81%)]\tLoss: 0.000228\n",
      "Train Epoch: 12 [24320/29236 (83%)]\tLoss: 0.002054\n",
      "Train Epoch: 12 [24960/29236 (85%)]\tLoss: 0.001259\n",
      "Train Epoch: 12 [25600/29236 (88%)]\tLoss: 0.000022\n",
      "Train Epoch: 12 [26240/29236 (90%)]\tLoss: 0.000721\n",
      "Train Epoch: 12 [26880/29236 (92%)]\tLoss: 0.000010\n",
      "Train Epoch: 12 [27520/29236 (94%)]\tLoss: 0.005879\n",
      "Train Epoch: 12 [28160/29236 (96%)]\tLoss: 0.024841\n",
      "Train Epoch: 12 [28800/29236 (98%)]\tLoss: 0.000047\n",
      "\n",
      "Test set: Average loss: 0.0150, Accuracy: 6044/7309 (83%)\n",
      "\n",
      "Epoch 12/14: Training Loss: 0.0001, Test Accuracy: 82.69%\n",
      "Train Epoch: 13 [0/29236 (0%)]\tLoss: 0.000116\n",
      "Train Epoch: 13 [640/29236 (2%)]\tLoss: 0.001380\n",
      "Train Epoch: 13 [1280/29236 (4%)]\tLoss: 0.004949\n",
      "Train Epoch: 13 [1920/29236 (7%)]\tLoss: 0.000172\n",
      "Train Epoch: 13 [2560/29236 (9%)]\tLoss: 0.000209\n",
      "Train Epoch: 13 [3200/29236 (11%)]\tLoss: 0.000196\n",
      "Train Epoch: 13 [3840/29236 (13%)]\tLoss: 0.021926\n",
      "Train Epoch: 13 [4480/29236 (15%)]\tLoss: 0.000051\n",
      "Train Epoch: 13 [5120/29236 (18%)]\tLoss: 0.003064\n",
      "Train Epoch: 13 [5760/29236 (20%)]\tLoss: 0.001181\n",
      "Train Epoch: 13 [6400/29236 (22%)]\tLoss: 0.000368\n",
      "Train Epoch: 13 [7040/29236 (24%)]\tLoss: 0.000333\n",
      "Train Epoch: 13 [7680/29236 (26%)]\tLoss: 0.000230\n",
      "Train Epoch: 13 [8320/29236 (28%)]\tLoss: 0.024957\n",
      "Train Epoch: 13 [8960/29236 (31%)]\tLoss: 0.001158\n",
      "Train Epoch: 13 [9600/29236 (33%)]\tLoss: 0.020768\n",
      "Train Epoch: 13 [10240/29236 (35%)]\tLoss: 0.000084\n",
      "Train Epoch: 13 [10880/29236 (37%)]\tLoss: 0.000120\n",
      "Train Epoch: 13 [11520/29236 (39%)]\tLoss: 0.000064\n",
      "Train Epoch: 13 [12160/29236 (42%)]\tLoss: 0.012574\n",
      "Train Epoch: 13 [12800/29236 (44%)]\tLoss: 0.000272\n",
      "Train Epoch: 13 [13440/29236 (46%)]\tLoss: 0.000143\n",
      "Train Epoch: 13 [14080/29236 (48%)]\tLoss: 0.000293\n",
      "Train Epoch: 13 [14720/29236 (50%)]\tLoss: 0.000444\n",
      "Train Epoch: 13 [15360/29236 (53%)]\tLoss: 0.000284\n",
      "Train Epoch: 13 [16000/29236 (55%)]\tLoss: 0.003057\n",
      "Train Epoch: 13 [16640/29236 (57%)]\tLoss: 0.000051\n",
      "Train Epoch: 13 [17280/29236 (59%)]\tLoss: 0.000315\n",
      "Train Epoch: 13 [17920/29236 (61%)]\tLoss: 0.001619\n",
      "Train Epoch: 13 [18560/29236 (63%)]\tLoss: 0.000495\n",
      "Train Epoch: 13 [19200/29236 (66%)]\tLoss: 0.020320\n",
      "Train Epoch: 13 [19840/29236 (68%)]\tLoss: 0.000076\n",
      "Train Epoch: 13 [20480/29236 (70%)]\tLoss: 0.000093\n",
      "Train Epoch: 13 [21120/29236 (72%)]\tLoss: 0.001857\n",
      "Train Epoch: 13 [21760/29236 (74%)]\tLoss: 0.000026\n",
      "Train Epoch: 13 [22400/29236 (77%)]\tLoss: 0.000009\n",
      "Train Epoch: 13 [23040/29236 (79%)]\tLoss: 0.001390\n",
      "Train Epoch: 13 [23680/29236 (81%)]\tLoss: 0.000195\n",
      "Train Epoch: 13 [24320/29236 (83%)]\tLoss: 0.026863\n",
      "Train Epoch: 13 [24960/29236 (85%)]\tLoss: 0.000065\n",
      "Train Epoch: 13 [25600/29236 (88%)]\tLoss: 0.000060\n",
      "Train Epoch: 13 [26240/29236 (90%)]\tLoss: 0.000042\n",
      "Train Epoch: 13 [26880/29236 (92%)]\tLoss: 0.000007\n",
      "Train Epoch: 13 [27520/29236 (94%)]\tLoss: 0.000344\n",
      "Train Epoch: 13 [28160/29236 (96%)]\tLoss: 0.042514\n",
      "Train Epoch: 13 [28800/29236 (98%)]\tLoss: 0.000011\n",
      "\n",
      "Test set: Average loss: 0.0148, Accuracy: 6043/7309 (83%)\n",
      "\n",
      "Epoch 13/14: Training Loss: 0.0001, Test Accuracy: 82.68%\n",
      "Train Epoch: 14 [0/29236 (0%)]\tLoss: 0.004487\n",
      "Train Epoch: 14 [640/29236 (2%)]\tLoss: 0.000304\n",
      "Train Epoch: 14 [1280/29236 (4%)]\tLoss: 0.000339\n",
      "Train Epoch: 14 [1920/29236 (7%)]\tLoss: 0.013682\n",
      "Train Epoch: 14 [2560/29236 (9%)]\tLoss: 0.003314\n",
      "Train Epoch: 14 [3200/29236 (11%)]\tLoss: 0.000138\n",
      "Train Epoch: 14 [3840/29236 (13%)]\tLoss: 0.003564\n",
      "Train Epoch: 14 [4480/29236 (15%)]\tLoss: 0.000318\n",
      "Train Epoch: 14 [5120/29236 (18%)]\tLoss: 0.010751\n",
      "Train Epoch: 14 [5760/29236 (20%)]\tLoss: 0.000399\n",
      "Train Epoch: 14 [6400/29236 (22%)]\tLoss: 0.000035\n",
      "Train Epoch: 14 [7040/29236 (24%)]\tLoss: 0.000023\n",
      "Train Epoch: 14 [7680/29236 (26%)]\tLoss: 0.000057\n",
      "Train Epoch: 14 [8320/29236 (28%)]\tLoss: 0.004881\n",
      "Train Epoch: 14 [8960/29236 (31%)]\tLoss: 0.002338\n",
      "Train Epoch: 14 [9600/29236 (33%)]\tLoss: 0.011304\n",
      "Train Epoch: 14 [10240/29236 (35%)]\tLoss: 0.000118\n",
      "Train Epoch: 14 [10880/29236 (37%)]\tLoss: 0.000115\n",
      "Train Epoch: 14 [11520/29236 (39%)]\tLoss: 0.000205\n",
      "Train Epoch: 14 [12160/29236 (42%)]\tLoss: 0.004030\n",
      "Train Epoch: 14 [12800/29236 (44%)]\tLoss: 0.000069\n",
      "Train Epoch: 14 [13440/29236 (46%)]\tLoss: 0.003524\n",
      "Train Epoch: 14 [14080/29236 (48%)]\tLoss: 0.000386\n",
      "Train Epoch: 14 [14720/29236 (50%)]\tLoss: 0.000507\n",
      "Train Epoch: 14 [15360/29236 (53%)]\tLoss: 0.000287\n",
      "Train Epoch: 14 [16000/29236 (55%)]\tLoss: 0.001185\n",
      "Train Epoch: 14 [16640/29236 (57%)]\tLoss: 0.000110\n",
      "Train Epoch: 14 [17280/29236 (59%)]\tLoss: 0.000167\n",
      "Train Epoch: 14 [17920/29236 (61%)]\tLoss: 0.005226\n",
      "Train Epoch: 14 [18560/29236 (63%)]\tLoss: 0.000141\n",
      "Train Epoch: 14 [19200/29236 (66%)]\tLoss: 0.001616\n",
      "Train Epoch: 14 [19840/29236 (68%)]\tLoss: 0.000079\n",
      "Train Epoch: 14 [20480/29236 (70%)]\tLoss: 0.000171\n",
      "Train Epoch: 14 [21120/29236 (72%)]\tLoss: 0.001686\n",
      "Train Epoch: 14 [21760/29236 (74%)]\tLoss: 0.002560\n",
      "Train Epoch: 14 [22400/29236 (77%)]\tLoss: 0.000049\n",
      "Train Epoch: 14 [23040/29236 (79%)]\tLoss: 0.000209\n",
      "Train Epoch: 14 [23680/29236 (81%)]\tLoss: 0.007702\n",
      "Train Epoch: 14 [24320/29236 (83%)]\tLoss: 0.001307\n",
      "Train Epoch: 14 [24960/29236 (85%)]\tLoss: 0.000127\n",
      "Train Epoch: 14 [25600/29236 (88%)]\tLoss: 0.000309\n",
      "Train Epoch: 14 [26240/29236 (90%)]\tLoss: 0.002020\n",
      "Train Epoch: 14 [26880/29236 (92%)]\tLoss: 0.000010\n",
      "Train Epoch: 14 [27520/29236 (94%)]\tLoss: 0.000335\n",
      "Train Epoch: 14 [28160/29236 (96%)]\tLoss: 0.045309\n",
      "Train Epoch: 14 [28800/29236 (98%)]\tLoss: 0.000097\n",
      "\n",
      "Test set: Average loss: 0.0146, Accuracy: 6044/7309 (83%)\n",
      "\n",
      "Epoch 14/14: Training Loss: 0.0001, Test Accuracy: 82.69%\n",
      "Model state dict saved to: models/mnist_cnn_task1.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_kwargs = {'batch_size': args.batch_size}\n",
    "test_kwargs = {'batch_size': args.test_batch_size}\n",
    "if 'cuda' in str(device):\n",
    "        cuda_kwargs = {'num_workers': 1, 'pin_memory': True, 'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "mnist_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "\n",
    "if args.test_reduced:\n",
    "        sampled_indices = torch.randperm(len(mnist_dataset))[:1000]\n",
    "        mnist_dataset = Subset(mnist_dataset, sampled_indices)\n",
    "\n",
    "# Define task 1 labeling\n",
    "task1_classes = [0, 1, 2, 3, 4, 9]\n",
    "# Define task 2 labeling\n",
    "task2_classes = [5, 6, 7, 8, 9, 1]\n",
    "\n",
    "task1_indices = [i for i, (_, label) in enumerate(mnist_dataset) if label in task1_classes]\n",
    "task2_indices = [i for i, (_, label) in enumerate(mnist_dataset) if label in task2_classes]\n",
    "\n",
    "task1_dataset = Subset(mnist_dataset, task1_indices)\n",
    "task2_dataset = Subset(mnist_dataset, task2_indices)\n",
    "\n",
    "train_size_task1 = int(0.8 * len(task1_indices))\n",
    "test_size_task1 = len(task1_indices) - train_size_task1\n",
    "\n",
    "train_size_task2 = int(0.8 * len(task2_indices))\n",
    "test_size_task2 = len(task2_indices) - train_size_task2\n",
    "\n",
    "train_loader_task1, test_loader_task1 = get_data_loaders(task1_dataset, train_size_task1, test_size_task1,\n",
    "                                                             train_kwargs, test_kwargs)\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "train_and_test(args, model, device, train_loader_task1, test_loader_task1, optimizer, scheduler,title=\"Task 1: \")\n",
    "model_state_dict_task1 = model.state_dict()\n",
    "save_model_state_dict(model_state_dict_task1, \"mnist_cnn_task1.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0146, Accuracy: 6044/7309 (83%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(82.69257080311944, 0.014609528788301586)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, device, test_loader_task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------MODEL TRAINING TASK #2-------------------\n",
      "Train Epoch: 1 [0/28916 (0%)]\tLoss: 2.667244\n",
      "Train Epoch: 1 [640/28916 (2%)]\tLoss: 0.017496\n",
      "Train Epoch: 1 [1280/28916 (4%)]\tLoss: 0.014989\n",
      "Train Epoch: 1 [1920/28916 (7%)]\tLoss: 0.003084\n",
      "Train Epoch: 1 [2560/28916 (9%)]\tLoss: 0.001027\n",
      "Train Epoch: 1 [3200/28916 (11%)]\tLoss: 0.001757\n",
      "Train Epoch: 1 [3840/28916 (13%)]\tLoss: 0.031435\n",
      "Train Epoch: 1 [4480/28916 (15%)]\tLoss: 0.004903\n",
      "Train Epoch: 1 [5120/28916 (18%)]\tLoss: 0.000152\n",
      "Train Epoch: 1 [5760/28916 (20%)]\tLoss: 0.092643\n",
      "Train Epoch: 1 [6400/28916 (22%)]\tLoss: 0.000362\n",
      "Train Epoch: 1 [7040/28916 (24%)]\tLoss: 0.000467\n",
      "Train Epoch: 1 [7680/28916 (27%)]\tLoss: 0.004624\n",
      "Train Epoch: 1 [8320/28916 (29%)]\tLoss: 0.000102\n",
      "Train Epoch: 1 [8960/28916 (31%)]\tLoss: 0.000009\n",
      "Train Epoch: 1 [9600/28916 (33%)]\tLoss: 0.025391\n",
      "Train Epoch: 1 [10240/28916 (35%)]\tLoss: 0.000477\n",
      "Train Epoch: 1 [10880/28916 (38%)]\tLoss: 0.008887\n",
      "Train Epoch: 1 [11520/28916 (40%)]\tLoss: 0.003277\n",
      "Train Epoch: 1 [12160/28916 (42%)]\tLoss: 0.000192\n",
      "Train Epoch: 1 [12800/28916 (44%)]\tLoss: 0.000715\n",
      "Train Epoch: 1 [13440/28916 (46%)]\tLoss: 0.000257\n",
      "Train Epoch: 1 [14080/28916 (49%)]\tLoss: 0.000984\n",
      "Train Epoch: 1 [14720/28916 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [15360/28916 (53%)]\tLoss: 0.004057\n",
      "Train Epoch: 1 [16000/28916 (55%)]\tLoss: 0.000239\n",
      "Train Epoch: 1 [16640/28916 (58%)]\tLoss: 0.004072\n",
      "Train Epoch: 1 [17280/28916 (60%)]\tLoss: 0.000007\n",
      "Train Epoch: 1 [17920/28916 (62%)]\tLoss: 0.000000\n",
      "Train Epoch: 1 [18560/28916 (64%)]\tLoss: 0.000016\n",
      "Train Epoch: 1 [19200/28916 (66%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [19840/28916 (69%)]\tLoss: 0.001078\n",
      "Train Epoch: 1 [20480/28916 (71%)]\tLoss: 0.000293\n",
      "Train Epoch: 1 [21120/28916 (73%)]\tLoss: 0.000518\n",
      "Train Epoch: 1 [21760/28916 (75%)]\tLoss: 0.000023\n",
      "Train Epoch: 1 [22400/28916 (77%)]\tLoss: 0.000001\n",
      "Train Epoch: 1 [23040/28916 (80%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [23680/28916 (82%)]\tLoss: 0.001026\n",
      "Train Epoch: 1 [24320/28916 (84%)]\tLoss: 0.024222\n",
      "Train Epoch: 1 [24960/28916 (86%)]\tLoss: 0.000282\n",
      "Train Epoch: 1 [25600/28916 (88%)]\tLoss: 0.000034\n",
      "Train Epoch: 1 [26240/28916 (91%)]\tLoss: 0.000407\n",
      "Train Epoch: 1 [26880/28916 (93%)]\tLoss: 0.115902\n",
      "Train Epoch: 1 [27520/28916 (95%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [28160/28916 (97%)]\tLoss: 0.002950\n",
      "Train Epoch: 1 [28800/28916 (100%)]\tLoss: 0.001931\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 2459/7230 (34%)\n",
      "\n",
      "Epoch 1/14: Training Loss: 0.0003, Test Accuracy: 34.01%\n",
      "Train Epoch: 2 [0/28916 (0%)]\tLoss: 0.000074\n",
      "Train Epoch: 2 [640/28916 (2%)]\tLoss: 0.000059\n",
      "Train Epoch: 2 [1280/28916 (4%)]\tLoss: 0.000014\n",
      "Train Epoch: 2 [1920/28916 (7%)]\tLoss: 0.000477\n",
      "Train Epoch: 2 [2560/28916 (9%)]\tLoss: 0.000211\n",
      "Train Epoch: 2 [3200/28916 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [3840/28916 (13%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [4480/28916 (15%)]\tLoss: 0.000989\n",
      "Train Epoch: 2 [5120/28916 (18%)]\tLoss: 0.000003\n",
      "Train Epoch: 2 [5760/28916 (20%)]\tLoss: 0.004654\n",
      "Train Epoch: 2 [6400/28916 (22%)]\tLoss: 0.000010\n",
      "Train Epoch: 2 [7040/28916 (24%)]\tLoss: 0.000004\n",
      "Train Epoch: 2 [7680/28916 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [8320/28916 (29%)]\tLoss: 0.000003\n",
      "Train Epoch: 2 [8960/28916 (31%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [9600/28916 (33%)]\tLoss: 0.005228\n",
      "Train Epoch: 2 [10240/28916 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [10880/28916 (38%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [11520/28916 (40%)]\tLoss: 0.000012\n",
      "Train Epoch: 2 [12160/28916 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [12800/28916 (44%)]\tLoss: 0.000021\n",
      "Train Epoch: 2 [13440/28916 (46%)]\tLoss: 0.000024\n",
      "Train Epoch: 2 [14080/28916 (49%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [14720/28916 (51%)]\tLoss: 0.027256\n",
      "Train Epoch: 2 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [16000/28916 (55%)]\tLoss: 0.001389\n",
      "Train Epoch: 2 [16640/28916 (58%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [17280/28916 (60%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [17920/28916 (62%)]\tLoss: 0.000009\n",
      "Train Epoch: 2 [18560/28916 (64%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [19200/28916 (66%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [19840/28916 (69%)]\tLoss: 0.000007\n",
      "Train Epoch: 2 [20480/28916 (71%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [21120/28916 (73%)]\tLoss: 0.000458\n",
      "Train Epoch: 2 [21760/28916 (75%)]\tLoss: 0.000034\n",
      "Train Epoch: 2 [22400/28916 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [23040/28916 (80%)]\tLoss: 0.000026\n",
      "Train Epoch: 2 [23680/28916 (82%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [24320/28916 (84%)]\tLoss: 0.014354\n",
      "Train Epoch: 2 [24960/28916 (86%)]\tLoss: 0.000030\n",
      "Train Epoch: 2 [25600/28916 (88%)]\tLoss: 0.000000\n",
      "Train Epoch: 2 [26240/28916 (91%)]\tLoss: 0.000026\n",
      "Train Epoch: 2 [26880/28916 (93%)]\tLoss: 0.171898\n",
      "Train Epoch: 2 [27520/28916 (95%)]\tLoss: 0.000064\n",
      "Train Epoch: 2 [28160/28916 (97%)]\tLoss: 0.000133\n",
      "Train Epoch: 2 [28800/28916 (100%)]\tLoss: 0.000025\n",
      "\n",
      "Test set: Average loss: 0.0007, Accuracy: 2456/7230 (34%)\n",
      "\n",
      "Epoch 2/14: Training Loss: 0.0000, Test Accuracy: 33.97%\n",
      "Train Epoch: 3 [0/28916 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [640/28916 (2%)]\tLoss: 0.000618\n",
      "Train Epoch: 3 [1280/28916 (4%)]\tLoss: 0.001227\n",
      "Train Epoch: 3 [1920/28916 (7%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [2560/28916 (9%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [3200/28916 (11%)]\tLoss: 0.000013\n",
      "Train Epoch: 3 [3840/28916 (13%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [4480/28916 (15%)]\tLoss: 0.001390\n",
      "Train Epoch: 3 [5120/28916 (18%)]\tLoss: 0.000011\n",
      "Train Epoch: 3 [5760/28916 (20%)]\tLoss: 0.000577\n",
      "Train Epoch: 3 [6400/28916 (22%)]\tLoss: 0.000044\n",
      "Train Epoch: 3 [7040/28916 (24%)]\tLoss: 0.000095\n",
      "Train Epoch: 3 [7680/28916 (27%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [8320/28916 (29%)]\tLoss: 0.015794\n",
      "Train Epoch: 3 [8960/28916 (31%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [9600/28916 (33%)]\tLoss: 0.000006\n",
      "Train Epoch: 3 [10240/28916 (35%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [10880/28916 (38%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [11520/28916 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [12160/28916 (42%)]\tLoss: 0.000046\n",
      "Train Epoch: 3 [12800/28916 (44%)]\tLoss: 0.000117\n",
      "Train Epoch: 3 [13440/28916 (46%)]\tLoss: 0.000004\n",
      "Train Epoch: 3 [14080/28916 (49%)]\tLoss: 0.000003\n",
      "Train Epoch: 3 [14720/28916 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [16000/28916 (55%)]\tLoss: 0.000013\n",
      "Train Epoch: 3 [16640/28916 (58%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [17920/28916 (62%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [18560/28916 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [19200/28916 (66%)]\tLoss: 0.000003\n",
      "Train Epoch: 3 [19840/28916 (69%)]\tLoss: 0.000014\n",
      "Train Epoch: 3 [20480/28916 (71%)]\tLoss: 0.000391\n",
      "Train Epoch: 3 [21120/28916 (73%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [21760/28916 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [22400/28916 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [23040/28916 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [23680/28916 (82%)]\tLoss: 0.000398\n",
      "Train Epoch: 3 [24320/28916 (84%)]\tLoss: 0.013458\n",
      "Train Epoch: 3 [24960/28916 (86%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [25600/28916 (88%)]\tLoss: 0.000000\n",
      "Train Epoch: 3 [26240/28916 (91%)]\tLoss: 0.000068\n",
      "Train Epoch: 3 [26880/28916 (93%)]\tLoss: 0.000007\n",
      "Train Epoch: 3 [27520/28916 (95%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [28160/28916 (97%)]\tLoss: 0.000011\n",
      "Train Epoch: 3 [28800/28916 (100%)]\tLoss: 0.000001\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 2457/7230 (34%)\n",
      "\n",
      "Epoch 3/14: Training Loss: 0.0000, Test Accuracy: 33.98%\n",
      "Train Epoch: 4 [0/28916 (0%)]\tLoss: 0.002170\n",
      "Train Epoch: 4 [640/28916 (2%)]\tLoss: 0.000029\n",
      "Train Epoch: 4 [1280/28916 (4%)]\tLoss: 0.000002\n",
      "Train Epoch: 4 [1920/28916 (7%)]\tLoss: 0.000005\n",
      "Train Epoch: 4 [2560/28916 (9%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [3200/28916 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [3840/28916 (13%)]\tLoss: 0.000683\n",
      "Train Epoch: 4 [4480/28916 (15%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [5120/28916 (18%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [5760/28916 (20%)]\tLoss: 0.000007\n",
      "Train Epoch: 4 [6400/28916 (22%)]\tLoss: 0.000001\n",
      "Train Epoch: 4 [7040/28916 (24%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [7680/28916 (27%)]\tLoss: 0.000725\n",
      "Train Epoch: 4 [8320/28916 (29%)]\tLoss: 0.000001\n",
      "Train Epoch: 4 [8960/28916 (31%)]\tLoss: 0.000010\n",
      "Train Epoch: 4 [9600/28916 (33%)]\tLoss: 0.000011\n",
      "Train Epoch: 4 [10240/28916 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [10880/28916 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [11520/28916 (40%)]\tLoss: 0.008461\n",
      "Train Epoch: 4 [12160/28916 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [12800/28916 (44%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [13440/28916 (46%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [14080/28916 (49%)]\tLoss: 0.000005\n",
      "Train Epoch: 4 [14720/28916 (51%)]\tLoss: 0.000124\n",
      "Train Epoch: 4 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [16000/28916 (55%)]\tLoss: 0.000017\n",
      "Train Epoch: 4 [16640/28916 (58%)]\tLoss: 0.000030\n",
      "Train Epoch: 4 [17280/28916 (60%)]\tLoss: 0.000280\n",
      "Train Epoch: 4 [17920/28916 (62%)]\tLoss: 0.000003\n",
      "Train Epoch: 4 [18560/28916 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [19200/28916 (66%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [19840/28916 (69%)]\tLoss: 0.000041\n",
      "Train Epoch: 4 [20480/28916 (71%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [21120/28916 (73%)]\tLoss: 0.000042\n",
      "Train Epoch: 4 [21760/28916 (75%)]\tLoss: 0.000135\n",
      "Train Epoch: 4 [22400/28916 (77%)]\tLoss: 0.000001\n",
      "Train Epoch: 4 [23040/28916 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [23680/28916 (82%)]\tLoss: 0.010480\n",
      "Train Epoch: 4 [24320/28916 (84%)]\tLoss: 0.000926\n",
      "Train Epoch: 4 [24960/28916 (86%)]\tLoss: 0.000005\n",
      "Train Epoch: 4 [25600/28916 (88%)]\tLoss: 0.000002\n",
      "Train Epoch: 4 [26240/28916 (91%)]\tLoss: 0.000012\n",
      "Train Epoch: 4 [26880/28916 (93%)]\tLoss: 0.001317\n",
      "Train Epoch: 4 [27520/28916 (95%)]\tLoss: 0.000000\n",
      "Train Epoch: 4 [28160/28916 (97%)]\tLoss: 0.000111\n",
      "Train Epoch: 4 [28800/28916 (100%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 2459/7230 (34%)\n",
      "\n",
      "Epoch 4/14: Training Loss: 0.0000, Test Accuracy: 34.01%\n",
      "Train Epoch: 5 [0/28916 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [640/28916 (2%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [1280/28916 (4%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [1920/28916 (7%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [2560/28916 (9%)]\tLoss: 0.000264\n",
      "Train Epoch: 5 [3200/28916 (11%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [3840/28916 (13%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [4480/28916 (15%)]\tLoss: 0.001149\n",
      "Train Epoch: 5 [5120/28916 (18%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [5760/28916 (20%)]\tLoss: 0.001903\n",
      "Train Epoch: 5 [6400/28916 (22%)]\tLoss: 0.000021\n",
      "Train Epoch: 5 [7040/28916 (24%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [7680/28916 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [8320/28916 (29%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [8960/28916 (31%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [9600/28916 (33%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [10240/28916 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [10880/28916 (38%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [11520/28916 (40%)]\tLoss: 0.000010\n",
      "Train Epoch: 5 [12160/28916 (42%)]\tLoss: 0.000504\n",
      "Train Epoch: 5 [12800/28916 (44%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [13440/28916 (46%)]\tLoss: 0.000022\n",
      "Train Epoch: 5 [14080/28916 (49%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [14720/28916 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [16000/28916 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [16640/28916 (58%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [17920/28916 (62%)]\tLoss: 0.000024\n",
      "Train Epoch: 5 [18560/28916 (64%)]\tLoss: 0.000011\n",
      "Train Epoch: 5 [19200/28916 (66%)]\tLoss: 0.000013\n",
      "Train Epoch: 5 [19840/28916 (69%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [20480/28916 (71%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [21120/28916 (73%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [21760/28916 (75%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [22400/28916 (77%)]\tLoss: 0.000027\n",
      "Train Epoch: 5 [23040/28916 (80%)]\tLoss: 0.000010\n",
      "Train Epoch: 5 [23680/28916 (82%)]\tLoss: 0.000002\n",
      "Train Epoch: 5 [24320/28916 (84%)]\tLoss: 0.000017\n",
      "Train Epoch: 5 [24960/28916 (86%)]\tLoss: 0.000000\n",
      "Train Epoch: 5 [25600/28916 (88%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [26240/28916 (91%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [26880/28916 (93%)]\tLoss: 0.027538\n",
      "Train Epoch: 5 [27520/28916 (95%)]\tLoss: 0.000018\n",
      "Train Epoch: 5 [28160/28916 (97%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [28800/28916 (100%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 2458/7230 (34%)\n",
      "\n",
      "Epoch 5/14: Training Loss: 0.0000, Test Accuracy: 34.00%\n",
      "Train Epoch: 6 [0/28916 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 6 [640/28916 (2%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [1280/28916 (4%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [1920/28916 (7%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [2560/28916 (9%)]\tLoss: 0.000146\n",
      "Train Epoch: 6 [3200/28916 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [3840/28916 (13%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [4480/28916 (15%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [5120/28916 (18%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [5760/28916 (20%)]\tLoss: 0.000024\n",
      "Train Epoch: 6 [6400/28916 (22%)]\tLoss: 0.000040\n",
      "Train Epoch: 6 [7040/28916 (24%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [7680/28916 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [8320/28916 (29%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [8960/28916 (31%)]\tLoss: 0.007517\n",
      "Train Epoch: 6 [9600/28916 (33%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [10240/28916 (35%)]\tLoss: 0.002104\n",
      "Train Epoch: 6 [10880/28916 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [11520/28916 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [12160/28916 (42%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [12800/28916 (44%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [13440/28916 (46%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [14080/28916 (49%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [14720/28916 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [16000/28916 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [16640/28916 (58%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [17920/28916 (62%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [18560/28916 (64%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [19200/28916 (66%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [19840/28916 (69%)]\tLoss: 0.003403\n",
      "Train Epoch: 6 [20480/28916 (71%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [21120/28916 (73%)]\tLoss: 0.000008\n",
      "Train Epoch: 6 [21760/28916 (75%)]\tLoss: 0.000019\n",
      "Train Epoch: 6 [22400/28916 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [23040/28916 (80%)]\tLoss: 0.000051\n",
      "Train Epoch: 6 [23680/28916 (82%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [24320/28916 (84%)]\tLoss: 0.000007\n",
      "Train Epoch: 6 [24960/28916 (86%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [25600/28916 (88%)]\tLoss: 0.000000\n",
      "Train Epoch: 6 [26240/28916 (91%)]\tLoss: 0.000078\n",
      "Train Epoch: 6 [26880/28916 (93%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [27520/28916 (95%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [28160/28916 (97%)]\tLoss: 0.000115\n",
      "Train Epoch: 6 [28800/28916 (100%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 2458/7230 (34%)\n",
      "\n",
      "Epoch 6/14: Training Loss: 0.0000, Test Accuracy: 34.00%\n",
      "Train Epoch: 7 [0/28916 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [640/28916 (2%)]\tLoss: 0.000020\n",
      "Train Epoch: 7 [1280/28916 (4%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [1920/28916 (7%)]\tLoss: 0.000135\n",
      "Train Epoch: 7 [2560/28916 (9%)]\tLoss: 0.000026\n",
      "Train Epoch: 7 [3200/28916 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [3840/28916 (13%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [4480/28916 (15%)]\tLoss: 0.000003\n",
      "Train Epoch: 7 [5120/28916 (18%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [5760/28916 (20%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [6400/28916 (22%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [7040/28916 (24%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [7680/28916 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [8320/28916 (29%)]\tLoss: 0.000891\n",
      "Train Epoch: 7 [8960/28916 (31%)]\tLoss: 0.000036\n",
      "Train Epoch: 7 [9600/28916 (33%)]\tLoss: 0.000091\n",
      "Train Epoch: 7 [10240/28916 (35%)]\tLoss: 0.000007\n",
      "Train Epoch: 7 [10880/28916 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [11520/28916 (40%)]\tLoss: 0.000065\n",
      "Train Epoch: 7 [12160/28916 (42%)]\tLoss: 0.000481\n",
      "Train Epoch: 7 [12800/28916 (44%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [13440/28916 (46%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [14080/28916 (49%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [14720/28916 (51%)]\tLoss: 0.000006\n",
      "Train Epoch: 7 [15360/28916 (53%)]\tLoss: 0.001195\n",
      "Train Epoch: 7 [16000/28916 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [16640/28916 (58%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [17920/28916 (62%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [18560/28916 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [19200/28916 (66%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [19840/28916 (69%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [20480/28916 (71%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [21120/28916 (73%)]\tLoss: 0.000024\n",
      "Train Epoch: 7 [21760/28916 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [22400/28916 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [23040/28916 (80%)]\tLoss: 0.000530\n",
      "Train Epoch: 7 [23680/28916 (82%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [24320/28916 (84%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [24960/28916 (86%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [25600/28916 (88%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [26240/28916 (91%)]\tLoss: 0.000022\n",
      "Train Epoch: 7 [26880/28916 (93%)]\tLoss: 0.001882\n",
      "Train Epoch: 7 [27520/28916 (95%)]\tLoss: 0.000471\n",
      "Train Epoch: 7 [28160/28916 (97%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [28800/28916 (100%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 2459/7230 (34%)\n",
      "\n",
      "Epoch 7/14: Training Loss: 0.0000, Test Accuracy: 34.01%\n",
      "Train Epoch: 8 [0/28916 (0%)]\tLoss: 0.000109\n",
      "Train Epoch: 8 [640/28916 (2%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [1280/28916 (4%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [1920/28916 (7%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [2560/28916 (9%)]\tLoss: 0.000096\n",
      "Train Epoch: 8 [3200/28916 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [3840/28916 (13%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [4480/28916 (15%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [5120/28916 (18%)]\tLoss: 0.000002\n",
      "Train Epoch: 8 [5760/28916 (20%)]\tLoss: 0.000008\n",
      "Train Epoch: 8 [6400/28916 (22%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [7040/28916 (24%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [7680/28916 (27%)]\tLoss: 0.000005\n",
      "Train Epoch: 8 [8320/28916 (29%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [8960/28916 (31%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [9600/28916 (33%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [10240/28916 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [10880/28916 (38%)]\tLoss: 0.000033\n",
      "Train Epoch: 8 [11520/28916 (40%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [12160/28916 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [12800/28916 (44%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [13440/28916 (46%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [14080/28916 (49%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [14720/28916 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [16000/28916 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [16640/28916 (58%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [17920/28916 (62%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [18560/28916 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [19200/28916 (66%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [19840/28916 (69%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [20480/28916 (71%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [21120/28916 (73%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [21760/28916 (75%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [22400/28916 (77%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [23040/28916 (80%)]\tLoss: 0.001426\n",
      "Train Epoch: 8 [23680/28916 (82%)]\tLoss: 0.000043\n",
      "Train Epoch: 8 [24320/28916 (84%)]\tLoss: 0.017589\n",
      "Train Epoch: 8 [24960/28916 (86%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [25600/28916 (88%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [26240/28916 (91%)]\tLoss: 0.000464\n",
      "Train Epoch: 8 [26880/28916 (93%)]\tLoss: 0.000005\n",
      "Train Epoch: 8 [27520/28916 (95%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [28160/28916 (97%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [28800/28916 (100%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 2459/7230 (34%)\n",
      "\n",
      "Epoch 8/14: Training Loss: 0.0000, Test Accuracy: 34.01%\n",
      "Train Epoch: 9 [0/28916 (0%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [640/28916 (2%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [1280/28916 (4%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [1920/28916 (7%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [2560/28916 (9%)]\tLoss: 0.000004\n",
      "Train Epoch: 9 [3200/28916 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [3840/28916 (13%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [4480/28916 (15%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [5120/28916 (18%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [5760/28916 (20%)]\tLoss: 0.000082\n",
      "Train Epoch: 9 [6400/28916 (22%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [7040/28916 (24%)]\tLoss: 0.000005\n",
      "Train Epoch: 9 [7680/28916 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [8320/28916 (29%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [8960/28916 (31%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [9600/28916 (33%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [10240/28916 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [10880/28916 (38%)]\tLoss: 0.000008\n",
      "Train Epoch: 9 [11520/28916 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [12160/28916 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [12800/28916 (44%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [13440/28916 (46%)]\tLoss: 0.000003\n",
      "Train Epoch: 9 [14080/28916 (49%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [14720/28916 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [15360/28916 (53%)]\tLoss: 0.026757\n",
      "Train Epoch: 9 [16000/28916 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [16640/28916 (58%)]\tLoss: 0.000011\n",
      "Train Epoch: 9 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [17920/28916 (62%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [18560/28916 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [19200/28916 (66%)]\tLoss: 0.000070\n",
      "Train Epoch: 9 [19840/28916 (69%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [20480/28916 (71%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [21120/28916 (73%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [21760/28916 (75%)]\tLoss: 0.000009\n",
      "Train Epoch: 9 [22400/28916 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [23040/28916 (80%)]\tLoss: 0.000004\n",
      "Train Epoch: 9 [23680/28916 (82%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [24320/28916 (84%)]\tLoss: 0.000004\n",
      "Train Epoch: 9 [24960/28916 (86%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [25600/28916 (88%)]\tLoss: 0.000039\n",
      "Train Epoch: 9 [26240/28916 (91%)]\tLoss: 0.004304\n",
      "Train Epoch: 9 [26880/28916 (93%)]\tLoss: 0.000003\n",
      "Train Epoch: 9 [27520/28916 (95%)]\tLoss: 0.000032\n",
      "Train Epoch: 9 [28160/28916 (97%)]\tLoss: 0.001848\n",
      "Train Epoch: 9 [28800/28916 (100%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 2459/7230 (34%)\n",
      "\n",
      "Epoch 9/14: Training Loss: 0.0000, Test Accuracy: 34.01%\n",
      "Train Epoch: 10 [0/28916 (0%)]\tLoss: 0.000013\n",
      "Train Epoch: 10 [640/28916 (2%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [1280/28916 (4%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [1920/28916 (7%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [2560/28916 (9%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [3200/28916 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [3840/28916 (13%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [4480/28916 (15%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [5120/28916 (18%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [5760/28916 (20%)]\tLoss: 0.000021\n",
      "Train Epoch: 10 [6400/28916 (22%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [7040/28916 (24%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [7680/28916 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [8320/28916 (29%)]\tLoss: 0.000061\n",
      "Train Epoch: 10 [8960/28916 (31%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [9600/28916 (33%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [10240/28916 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [10880/28916 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [11520/28916 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [12160/28916 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [12800/28916 (44%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [13440/28916 (46%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [14080/28916 (49%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [14720/28916 (51%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [16000/28916 (55%)]\tLoss: 0.001849\n",
      "Train Epoch: 10 [16640/28916 (58%)]\tLoss: 0.000196\n",
      "Train Epoch: 10 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [17920/28916 (62%)]\tLoss: 0.000177\n",
      "Train Epoch: 10 [18560/28916 (64%)]\tLoss: 0.000003\n",
      "Train Epoch: 10 [19200/28916 (66%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [19840/28916 (69%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [20480/28916 (71%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [21120/28916 (73%)]\tLoss: 0.000005\n",
      "Train Epoch: 10 [21760/28916 (75%)]\tLoss: 0.000344\n",
      "Train Epoch: 10 [22400/28916 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [23040/28916 (80%)]\tLoss: 0.000051\n",
      "Train Epoch: 10 [23680/28916 (82%)]\tLoss: 0.000090\n",
      "Train Epoch: 10 [24320/28916 (84%)]\tLoss: 0.000599\n",
      "Train Epoch: 10 [24960/28916 (86%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [25600/28916 (88%)]\tLoss: 0.000004\n",
      "Train Epoch: 10 [26240/28916 (91%)]\tLoss: 0.000612\n",
      "Train Epoch: 10 [26880/28916 (93%)]\tLoss: 0.000003\n",
      "Train Epoch: 10 [27520/28916 (95%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [28160/28916 (97%)]\tLoss: 0.000006\n",
      "Train Epoch: 10 [28800/28916 (100%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 2459/7230 (34%)\n",
      "\n",
      "Epoch 10/14: Training Loss: 0.0000, Test Accuracy: 34.01%\n",
      "Train Epoch: 11 [0/28916 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 11 [640/28916 (2%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [1280/28916 (4%)]\tLoss: 0.000001\n",
      "Train Epoch: 11 [1920/28916 (7%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [2560/28916 (9%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [3200/28916 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [3840/28916 (13%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [4480/28916 (15%)]\tLoss: 0.000214\n",
      "Train Epoch: 11 [5120/28916 (18%)]\tLoss: 0.000007\n",
      "Train Epoch: 11 [5760/28916 (20%)]\tLoss: 0.000001\n",
      "Train Epoch: 11 [6400/28916 (22%)]\tLoss: 0.000003\n",
      "Train Epoch: 11 [7040/28916 (24%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [7680/28916 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [8320/28916 (29%)]\tLoss: 0.000012\n",
      "Train Epoch: 11 [8960/28916 (31%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [9600/28916 (33%)]\tLoss: 0.000003\n",
      "Train Epoch: 11 [10240/28916 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [10880/28916 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [11520/28916 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [12160/28916 (42%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [12800/28916 (44%)]\tLoss: 0.000003\n",
      "Train Epoch: 11 [13440/28916 (46%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [14080/28916 (49%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [14720/28916 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [16000/28916 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [16640/28916 (58%)]\tLoss: 0.000005\n",
      "Train Epoch: 11 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [17920/28916 (62%)]\tLoss: 0.000001\n",
      "Train Epoch: 11 [18560/28916 (64%)]\tLoss: 0.000006\n",
      "Train Epoch: 11 [19200/28916 (66%)]\tLoss: 0.000012\n",
      "Train Epoch: 11 [19840/28916 (69%)]\tLoss: 0.000002\n",
      "Train Epoch: 11 [20480/28916 (71%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [21120/28916 (73%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [21760/28916 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [22400/28916 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [23040/28916 (80%)]\tLoss: 0.000107\n",
      "Train Epoch: 11 [23680/28916 (82%)]\tLoss: 0.000050\n",
      "Train Epoch: 11 [24320/28916 (84%)]\tLoss: 0.000009\n",
      "Train Epoch: 11 [24960/28916 (86%)]\tLoss: 0.000012\n",
      "Train Epoch: 11 [25600/28916 (88%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [26240/28916 (91%)]\tLoss: 0.000454\n",
      "Train Epoch: 11 [26880/28916 (93%)]\tLoss: 0.000197\n",
      "Train Epoch: 11 [27520/28916 (95%)]\tLoss: 0.000008\n",
      "Train Epoch: 11 [28160/28916 (97%)]\tLoss: 0.000000\n",
      "Train Epoch: 11 [28800/28916 (100%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 2459/7230 (34%)\n",
      "\n",
      "Epoch 11/14: Training Loss: 0.0000, Test Accuracy: 34.01%\n",
      "Train Epoch: 12 [0/28916 (0%)]\tLoss: 0.000438\n",
      "Train Epoch: 12 [640/28916 (2%)]\tLoss: 0.000020\n",
      "Train Epoch: 12 [1280/28916 (4%)]\tLoss: 0.000006\n",
      "Train Epoch: 12 [1920/28916 (7%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [2560/28916 (9%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [3200/28916 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [3840/28916 (13%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [4480/28916 (15%)]\tLoss: 0.000001\n",
      "Train Epoch: 12 [5120/28916 (18%)]\tLoss: 0.024632\n",
      "Train Epoch: 12 [5760/28916 (20%)]\tLoss: 0.000032\n",
      "Train Epoch: 12 [6400/28916 (22%)]\tLoss: 0.000001\n",
      "Train Epoch: 12 [7040/28916 (24%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [7680/28916 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [8320/28916 (29%)]\tLoss: 0.000010\n",
      "Train Epoch: 12 [8960/28916 (31%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [9600/28916 (33%)]\tLoss: 0.000003\n",
      "Train Epoch: 12 [10240/28916 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [10880/28916 (38%)]\tLoss: 0.000003\n",
      "Train Epoch: 12 [11520/28916 (40%)]\tLoss: 0.000002\n",
      "Train Epoch: 12 [12160/28916 (42%)]\tLoss: 0.000108\n",
      "Train Epoch: 12 [12800/28916 (44%)]\tLoss: 0.000006\n",
      "Train Epoch: 12 [13440/28916 (46%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [14080/28916 (49%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [14720/28916 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [16000/28916 (55%)]\tLoss: 0.000002\n",
      "Train Epoch: 12 [16640/28916 (58%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [17920/28916 (62%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [18560/28916 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [19200/28916 (66%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [19840/28916 (69%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [20480/28916 (71%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [21120/28916 (73%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [21760/28916 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [22400/28916 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [23040/28916 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [23680/28916 (82%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [24320/28916 (84%)]\tLoss: 0.000879\n",
      "Train Epoch: 12 [24960/28916 (86%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [25600/28916 (88%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [26240/28916 (91%)]\tLoss: 0.004176\n",
      "Train Epoch: 12 [26880/28916 (93%)]\tLoss: 0.063456\n",
      "Train Epoch: 12 [27520/28916 (95%)]\tLoss: 0.000003\n",
      "Train Epoch: 12 [28160/28916 (97%)]\tLoss: 0.000000\n",
      "Train Epoch: 12 [28800/28916 (100%)]\tLoss: 0.000056\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 2459/7230 (34%)\n",
      "\n",
      "Epoch 12/14: Training Loss: 0.0000, Test Accuracy: 34.01%\n",
      "Train Epoch: 13 [0/28916 (0%)]\tLoss: 0.000045\n",
      "Train Epoch: 13 [640/28916 (2%)]\tLoss: 0.000002\n",
      "Train Epoch: 13 [1280/28916 (4%)]\tLoss: 0.000039\n",
      "Train Epoch: 13 [1920/28916 (7%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [2560/28916 (9%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [3200/28916 (11%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [3840/28916 (13%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [4480/28916 (15%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [5120/28916 (18%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [5760/28916 (20%)]\tLoss: 0.000001\n",
      "Train Epoch: 13 [6400/28916 (22%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [7040/28916 (24%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [7680/28916 (27%)]\tLoss: 0.000041\n",
      "Train Epoch: 13 [8320/28916 (29%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [8960/28916 (31%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [9600/28916 (33%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [10240/28916 (35%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [10880/28916 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [11520/28916 (40%)]\tLoss: 0.000004\n",
      "Train Epoch: 13 [12160/28916 (42%)]\tLoss: 0.000007\n",
      "Train Epoch: 13 [12800/28916 (44%)]\tLoss: 0.000010\n",
      "Train Epoch: 13 [13440/28916 (46%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [14080/28916 (49%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [14720/28916 (51%)]\tLoss: 0.000002\n",
      "Train Epoch: 13 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [16000/28916 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [16640/28916 (58%)]\tLoss: 0.000003\n",
      "Train Epoch: 13 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [17920/28916 (62%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [18560/28916 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [19200/28916 (66%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [19840/28916 (69%)]\tLoss: 0.000001\n",
      "Train Epoch: 13 [20480/28916 (71%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [21120/28916 (73%)]\tLoss: 0.000051\n",
      "Train Epoch: 13 [21760/28916 (75%)]\tLoss: 0.000030\n",
      "Train Epoch: 13 [22400/28916 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [23040/28916 (80%)]\tLoss: 0.000001\n",
      "Train Epoch: 13 [23680/28916 (82%)]\tLoss: 0.000065\n",
      "Train Epoch: 13 [24320/28916 (84%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [24960/28916 (86%)]\tLoss: 0.000151\n",
      "Train Epoch: 13 [25600/28916 (88%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [26240/28916 (91%)]\tLoss: 0.000038\n",
      "Train Epoch: 13 [26880/28916 (93%)]\tLoss: 0.000234\n",
      "Train Epoch: 13 [27520/28916 (95%)]\tLoss: 0.000000\n",
      "Train Epoch: 13 [28160/28916 (97%)]\tLoss: 0.000003\n",
      "Train Epoch: 13 [28800/28916 (100%)]\tLoss: 0.000000\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 2459/7230 (34%)\n",
      "\n",
      "Epoch 13/14: Training Loss: 0.0000, Test Accuracy: 34.01%\n",
      "Train Epoch: 14 [0/28916 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 14 [640/28916 (2%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [1280/28916 (4%)]\tLoss: 0.000031\n",
      "Train Epoch: 14 [1920/28916 (7%)]\tLoss: 0.000001\n",
      "Train Epoch: 14 [2560/28916 (9%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [3200/28916 (11%)]\tLoss: 0.000002\n",
      "Train Epoch: 14 [3840/28916 (13%)]\tLoss: 0.000052\n",
      "Train Epoch: 14 [4480/28916 (15%)]\tLoss: 0.000005\n",
      "Train Epoch: 14 [5120/28916 (18%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [5760/28916 (20%)]\tLoss: 0.000001\n",
      "Train Epoch: 14 [6400/28916 (22%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [7040/28916 (24%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [7680/28916 (27%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [8320/28916 (29%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [8960/28916 (31%)]\tLoss: 0.000003\n",
      "Train Epoch: 14 [9600/28916 (33%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [10240/28916 (35%)]\tLoss: 0.000030\n",
      "Train Epoch: 14 [10880/28916 (38%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [11520/28916 (40%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [12160/28916 (42%)]\tLoss: 0.000792\n",
      "Train Epoch: 14 [12800/28916 (44%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [13440/28916 (46%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [14080/28916 (49%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [14720/28916 (51%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [15360/28916 (53%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [16000/28916 (55%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [16640/28916 (58%)]\tLoss: 0.000220\n",
      "Train Epoch: 14 [17280/28916 (60%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [17920/28916 (62%)]\tLoss: 0.000005\n",
      "Train Epoch: 14 [18560/28916 (64%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [19200/28916 (66%)]\tLoss: 0.000001\n",
      "Train Epoch: 14 [19840/28916 (69%)]\tLoss: 0.000004\n",
      "Train Epoch: 14 [20480/28916 (71%)]\tLoss: 0.000232\n",
      "Train Epoch: 14 [21120/28916 (73%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [21760/28916 (75%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [22400/28916 (77%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [23040/28916 (80%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [23680/28916 (82%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [24320/28916 (84%)]\tLoss: 0.000007\n",
      "Train Epoch: 14 [24960/28916 (86%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [25600/28916 (88%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [26240/28916 (91%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [26880/28916 (93%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [27520/28916 (95%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [28160/28916 (97%)]\tLoss: 0.000000\n",
      "Train Epoch: 14 [28800/28916 (100%)]\tLoss: 0.000012\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 2459/7230 (34%)\n",
      "\n",
      "Epoch 14/14: Training Loss: 0.0000, Test Accuracy: 34.01%\n",
      "Model state dict saved to: models/mnist_cnn_task2.pt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_task2 = Net().to(device)\n",
    "model_task2.load_state_dict(model_state_dict_task1.copy()) \n",
    "optimizer_task2 = optim.Adadelta(model_task2.parameters(), lr=args.lr)\n",
    "scheduler_task2 = StepLR(optimizer_task2, step_size=1, gamma=args.gamma)\n",
    "\n",
    "print(f\"MODEL TRAINING TASK #2\".center(60, \"-\"))\n",
    "\n",
    "train_loader_task2, test_loader_task2 = get_data_loaders(task2_dataset, train_size_task2, test_size_task2,\n",
    "                                                             train_kwargs, test_kwargs)\n",
    "\n",
    "train_and_test(args, model_task2, device, train_loader_task2, test_loader_task2, optimizer_task2, scheduler_task2, title=\"Task 2: \")\n",
    "\n",
    "model_state_dict_task2 = model_task2.state_dict()\n",
    "save_model_state_dict(model_state_dict_task1, \"mnist_cnn_task2.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model again on task 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 8.0510, Accuracy: 1848/7309 (25%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25.283896565877686, 8.0509697082578)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_task1_adapted = Net().to(device)\n",
    "model_task1_adapted.load_state_dict(model_state_dict_task2.copy()) \n",
    "optimizer_task1_adapted = optim.Adadelta(model_task1_adapted.parameters(), lr=args.lr)\n",
    "scheduler_taskadapted = StepLR(optimizer_task2, step_size=1, gamma=args.gamma)\n",
    "train_loader_task1_adapted, test_loader_task1_adapted = get_data_loaders(task1_dataset, train_size_task1, test_size_task1,\n",
    "                                                             train_kwargs, test_kwargs)\n",
    "# train_and_test(args, model_task2, device, train_loader_task2, test_loader_task2, optimizer_task2, scheduler_task2, title=\"Task 2: \")\n",
    "test(model_task1_adapted, device, test_loader_task1_adapted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
